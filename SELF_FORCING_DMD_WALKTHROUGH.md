# Self-Forcing Training with DMD ä»£ç æ‰§è¡Œæµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†æ¢³ç†äº† **Self-Forcing Training with DMD** çš„ä»£ç æ‰§è¡Œè·¯å¾„ï¼Œå¸®åŠ©ä½ ç†è§£ä»è®­ç»ƒå¯åŠ¨åˆ°æ ¸å¿ƒç®—æ³•è®¡ç®—çš„å®Œæ•´è¿‡ç¨‹ã€‚

## ğŸ“‹ ç›®å½•

1. [æ€»ä½“æ¶æ„æ¦‚è§ˆ](#1-æ€»ä½“æ¶æ„æ¦‚è§ˆ)
2. [å¯åŠ¨å‘½ä»¤è§£æ](#2-å¯åŠ¨å‘½ä»¤è§£æ)
3. [å…¥å£æ–‡ä»¶ train.py](#3-å…¥å£æ–‡ä»¶-trainpy)
4. [Trainer åˆå§‹åŒ–](#4-trainer-åˆå§‹åŒ–)
5. [è®­ç»ƒä¸»å¾ªç¯](#5-è®­ç»ƒä¸»å¾ªç¯)
6. [æ ¸å¿ƒç®—æ³•ï¼šSelf-Forcing + DMD](#6-æ ¸å¿ƒç®—æ³•self-forcing--dmd)
7. [å…³é”®ç»„ä»¶è¯¦è§£](#7-å…³é”®ç»„ä»¶è¯¦è§£)

---

## 1. æ€»ä½“æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è®­ç»ƒå¯åŠ¨å‘½ä»¤                               â”‚
â”‚  torchrun train.py --config_path configs/self_forcing_dmd.yaml  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        train.py (å…¥å£)                           â”‚
â”‚  1. è§£æå‘½ä»¤è¡Œå‚æ•°                                                â”‚
â”‚  2. åŠ è½½é…ç½®æ–‡ä»¶ (default + self_forcing_dmd.yaml)               â”‚
â”‚  3. é€‰æ‹© Trainer ç±»å‹ â†’ ScoreDistillationTrainer                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              trainer/distillation.py (Trainerç±»)                â”‚
â”‚  1. åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ (FSDP)                                   â”‚
â”‚  2. åˆå§‹åŒ–æ¨¡å‹ (DMD)                                             â”‚
â”‚  3. åˆå§‹åŒ–ä¼˜åŒ–å™¨ (Generator + Critic)                            â”‚
â”‚  4. æ‰§è¡Œè®­ç»ƒå¾ªç¯                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    model/dmd.py (DMDæ¨¡å‹)                        â”‚
â”‚  åŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç½‘ç»œ:                                                â”‚
â”‚  â€¢ Generator (ç”Ÿæˆå™¨) - å°‘æ­¥æ•°å»å™ª                                â”‚
â”‚  â€¢ Real Score (çœŸå®è¯„åˆ†ç½‘ç»œ) - æ¥è‡ªé¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹                  â”‚
â”‚  â€¢ Fake Score (ä¼ªé€ è¯„åˆ†ç½‘ç»œ) - å­¦ä¹ ç”Ÿæˆå™¨åˆ†å¸ƒ                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         pipeline/self_forcing_training.py (è‡ªå›å½’æ¨ç†)           â”‚
â”‚  â€¢ å®ç° Self-Forcing çš„æ ¸å¿ƒ: è®­ç»ƒæ—¶æ¨¡æ‹Ÿæ¨ç†è¿‡ç¨‹                    â”‚
â”‚  â€¢ KV Cache ç®¡ç†                                                 â”‚
â”‚  â€¢ é€ Block è‡ªå›å½’ç”Ÿæˆ                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. å¯åŠ¨å‘½ä»¤è§£æ

æ ¹æ® READMEï¼Œè®­ç»ƒå‘½ä»¤å¦‚ä¸‹ï¼š

```bash
torchrun --nnodes=8 --nproc_per_node=8 --rdzv_id=5235 \
  --rdzv_backend=c10d \
  --rdzv_endpoint $MASTER_ADDR \
  train.py \
  --config_path configs/self_forcing_dmd.yaml \
  --logdir logs/self_forcing_dmd \
  --disable-wandb
```

| å‚æ•° | è¯´æ˜ |
|------|------|
| `--nnodes=8` | ä½¿ç”¨ 8 ä¸ªèŠ‚ç‚¹ (æœºå™¨) |
| `--nproc_per_node=8` | æ¯ä¸ªèŠ‚ç‚¹ 8 ä¸ª GPU |
| `--rdzv_backend=c10d` | PyTorch åˆ†å¸ƒå¼é€šä¿¡åç«¯ |
| `--config_path` | è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„ (æ ¸å¿ƒå‚æ•°) |
| `--logdir` | æ—¥å¿—å’Œæ¨¡å‹ä¿å­˜ç›®å½• |
| `--disable-wandb` | ç¦ç”¨ Weights & Biases è®°å½• |

---

## 3. å…¥å£æ–‡ä»¶ train.py

æ ¸å¿ƒé€»è¾‘ä½äº `train.py`ï¼š

```python
def main():
    # Step 1: è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--config_path", type=str, required=True)
    args = parser.parse_args()

    # Step 2: åŠ è½½å¹¶åˆå¹¶é…ç½®
    config = OmegaConf.load(args.config_path)
    default_config = OmegaConf.load("configs/default_config.yaml")
    config = OmegaConf.merge(default_config, config)  # åˆå¹¶é…ç½®

    # Step 3: æ ¹æ® config.trainer é€‰æ‹© Trainer ç±»å‹
    # é…ç½®æ–‡ä»¶ä¸­ trainer: score_distillation
    if config.trainer == "score_distillation":
        trainer = ScoreDistillationTrainer(config)  # â† å®ä¾‹åŒ– DMD è®­ç»ƒå™¨
    
    # Step 4: å¼€å§‹è®­ç»ƒ
    trainer.train()
```

**å…³é”®ç‚¹**: é…ç½®æ–‡ä»¶ `self_forcing_dmd.yaml` ä¸­è®¾ç½®äº† `trainer: score_distillation`ï¼Œè¿™å†³å®šäº†ç³»ç»Ÿå°†ä½¿ç”¨ `trainer/distillation.py` ä¸­çš„ `Trainer` ç±»ã€‚

---

## 4. Trainer åˆå§‹åŒ–

ä½äº `trainer/distillation.py` ä¸­çš„ `__init__` æ–¹æ³•ï¼š

```python
class Trainer:
    def __init__(self, config):
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        launch_distributed_job()  # å¯åŠ¨åˆ†å¸ƒå¼è¿›ç¨‹ç»„
        global_rank = dist.get_rank()
        self.world_size = dist.get_world_size()  # æ€» GPU æ•° (64)
        self.device = torch.cuda.current_device()
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: åˆå§‹åŒ– DMD æ¨¡å‹
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # å› ä¸º config.distribution_loss == "dmd"
        self.model = DMD(config, device=self.device)
        
        # DMD æ¨¡å‹å†…éƒ¨åŒ…å«:
        # - self.model.generator      (å°‘æ­¥æ•°ç”Ÿæˆå™¨, éœ€è¦è®­ç»ƒ)
        # - self.model.real_score     (æ•™å¸ˆæ¨¡å‹, å†»ç»“)
        # - self.model.fake_score     (Critic, éœ€è¦è®­ç»ƒ)
        # - self.model.text_encoder   (T5æ–‡æœ¬ç¼–ç å™¨, å†»ç»“)
        # - self.model.vae            (è§†é¢‘VAE, å†»ç»“)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 3: ä½¿ç”¨ FSDP åŒ…è£…æ¨¡å‹ (åˆ†å¸ƒå¼è®­ç»ƒ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.model.generator = fsdp_wrap(self.model.generator, ...)
        self.model.real_score = fsdp_wrap(self.model.real_score, ...)
        self.model.fake_score = fsdp_wrap(self.model.fake_score, ...)
        self.model.text_encoder = fsdp_wrap(self.model.text_encoder, ...)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 4: åˆå§‹åŒ–ä¼˜åŒ–å™¨
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # æ³¨æ„æœ‰ä¸¤ä¸ªä¼˜åŒ–å™¨ï¼šä¸€ä¸ªç”¨äºç”Ÿæˆå™¨ï¼Œä¸€ä¸ªç”¨äºCritic
        self.generator_optimizer = torch.optim.AdamW(
            self.model.generator.parameters(), lr=config.lr)
        self.critic_optimizer = torch.optim.AdamW(
            self.model.fake_score.parameters(), lr=config.lr_critic)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 5: åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Self-Forcing æ˜¯ Data-Free çš„! åªéœ€è¦åŠ è½½çº¯æ–‡æœ¬ prompts
        dataset = TextDataset(config.data_path)
        self.dataloader = cycle(DataLoader(dataset, ...))
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 6: åŠ è½½é¢„è®­ç»ƒæƒé‡ (ODE åˆå§‹åŒ–)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        state_dict = torch.load(config.generator_ckpt)
        self.model.generator.load_state_dict(state_dict)
```

---

## 5. è®­ç»ƒä¸»å¾ªç¯

ä½äº `trainer/distillation.py` ä¸­çš„ `train()` æ–¹æ³•ï¼š

```python
def train(self):
    while True:
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # äº¤æ›¿è®­ç»ƒç­–ç•¥: æ¯ 5 æ­¥è®­ç»ƒ 1 æ¬¡ç”Ÿæˆå™¨, æ¯æ­¥éƒ½è®­ç»ƒ Critic
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        TRAIN_GENERATOR = (self.step % 5 == 0)  # dfake_gen_update_ratio=5
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Part A: è®­ç»ƒç”Ÿæˆå™¨ (æ¯ 5 æ­¥ä¸€æ¬¡)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if TRAIN_GENERATOR:
            self.generator_optimizer.zero_grad()
            batch = next(self.dataloader)  # è·å–æ–‡æœ¬ prompts
            
            # æ ¸å¿ƒ: å‰å‘ä¼ æ’­ + è®¡ç®—æŸå¤± + åå‘ä¼ æ’­
            # fwdbwd_one_step ä¼šè°ƒç”¨ model.generator_loss
            generator_log_dict = self.fwdbwd_one_step(batch, train_generator=True)
            
            self.generator_optimizer.step()
            
            # EMA (Exponential Moving Average) æ›´æ–°
            if self.generator_ema is not None:
                self.generator_ema.update(self.model.generator)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Part B: è®­ç»ƒ Critic (æ¯æ­¥éƒ½è®­ç»ƒ)
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.critic_optimizer.zero_grad()
        batch = next(self.dataloader)
        
        # fwdbwd_one_step ä¼šè°ƒç”¨ model.critic_loss
        critic_log_dict = self.fwdbwd_one_step(batch, train_generator=False)
        
        self.critic_optimizer.step()
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Part C: ä¿å­˜æ¨¡å‹ & æ—¥å¿—
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.step += 1
        if self.step % 50 == 0:  # log_iters=50
            self.save()
```

---

## 6. æ ¸å¿ƒç®—æ³•ï¼šSelf-Forcing + DMD

### 6.1 ç”Ÿæˆå™¨æŸå¤±è®¡ç®— (`model/dmd.py`)

è¿™éƒ¨åˆ†ä»£ç å®ç°äº† Self-Forcing çš„æ ¸å¿ƒé€»è¾‘ï¼š

```python
def generator_loss(self, image_or_video_shape, conditional_dict, ...):
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 1: è¿è¡Œç”Ÿæˆå™¨ (Self-Forcing è‡ªå›å½’ç”Ÿæˆ)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # å…³é”®ç‚¹: è®­ç»ƒæ—¶æ¨¡æ‹Ÿå®Œæ•´çš„æ¨ç†è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ Ground Truth è§†é¢‘!
    # è¿™è°ƒç”¨äº† _consistency_backward_simulation -> inference_with_trajectory
    pred_image, gradient_mask, timestep_from, timestep_to = self._run_generator(
        image_or_video_shape=image_or_video_shape,
        conditional_dict=conditional_dict
    )
    # pred_image: [B, 21, 16, 60, 104] - ç”Ÿæˆçš„è§†é¢‘ Latent
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Step 2: è®¡ç®— DMD Loss
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # è®¡ç®—ç”Ÿæˆå†…å®¹ä¸çœŸå®åˆ†å¸ƒ(æ•™å¸ˆæ¨¡å‹)ä¹‹é—´çš„è·ç¦»
    dmd_loss, dmd_log_dict = self.compute_distribution_matching_loss(
        image_or_video=pred_image,
        conditional_dict=conditional_dict,
        unconditional_dict=unconditional_dict
    )
    
    return dmd_loss, dmd_log_dict
```

### 6.2 Self-Forcing ç”Ÿæˆè¿‡ç¨‹è¯¦è§£ (`pipeline/self_forcing_training.py`)

è¿™æ˜¯é¡¹ç›®ä¸­**æœ€æ ¸å¿ƒ**çš„æ–‡ä»¶ä¹‹ä¸€ï¼Œå®ƒåœ¨è®­ç»ƒæ—¶å®ç°äº†å¸¦ KV Cache çš„è‡ªå›å½’ç”Ÿæˆï¼š

```python
class SelfForcingTrainingPipeline:
    def inference_with_trajectory(self, noise, **conditional_dict):
        batch_size, num_frames, C, H, W = noise.shape
        num_blocks = num_frames // self.num_frame_per_block  # 21 // 3 = 7 blocks
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 1: åˆå§‹åŒ– KV Cache
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._initialize_kv_cache(batch_size, dtype, device)
        # kv_cache ç”¨äºç¼“å­˜å‰é¢å¸§çš„ Attention Key/Valueï¼Œé¿å…é‡å¤è®¡ç®—
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Step 2: é€ Block è‡ªå›å½’ç”Ÿæˆ
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        for block_index in range(num_blocks):  # 0, 1, ..., 6
            # è·å–å½“å‰ Block çš„è¾“å…¥å™ªå£° (3 å¸§)
            noisy_input = noise[:, start:start+3, ...]
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Step 2.1: ç©ºé—´å»å™ªå¾ªç¯ (4æ­¥: 1000â†’750â†’500â†’250)
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            for index, current_timestep in enumerate(self.denoising_step_list):
                # denoising_step_list = [1000, 750, 500, 250]
                
                # éšæœºé€‰æ‹©ä¸€ä¸ªæ­¥éª¤è®¡ç®—æ¢¯åº¦ (ç”¨äºåå‘ä¼ æ’­)ï¼Œå…¶ä»–æ­¥éª¤ä»…æ¨ç†
                exit_flag = (index == exit_flags[block_index])
                
                if not exit_flag:
                    # éæ¢¯åº¦æ­¥: åªåšæ¨ç†
                    with torch.no_grad():
                        _, denoised_pred = self.generator(
                            noisy_image_or_video=noisy_input,
                            conditional_dict=conditional_dict,
                            timestep=current_timestep,
                            kv_cache=self.kv_cache1,  # â† ä½¿ç”¨å¹¶æ›´æ–° KV Cache
                            current_start=current_start_frame * 1560
                        )
                        # æ·»åŠ å™ªå£°ç”¨äºä¸‹ä¸€æ­¥ (Langevin Dynamics æˆ–ç±»ä¼¼é€»è¾‘)
                        noisy_input = scheduler.add_noise(denoised_pred, ...)
                else:
                    # æ¢¯åº¦æ­¥: æ­£å¸¸è®¡ç®—å›¾ï¼Œä¿ç•™æ¢¯åº¦
                    _, denoised_pred = self.generator(...)
                    break  # ç›´æ¥é€€å‡ºå†…å±‚å¾ªç¯ï¼Œä½¿ç”¨å½“å‰é¢„æµ‹ä½œä¸ºè¯¥ Block çš„è¾“å‡º
            
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Step 2.2: è®°å½•å¹¶ç¼“å­˜
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            output[:, start:start+3] = denoised_pred
            
            # å…³é”®: ç”¨ timestep=0 (Clean) é‡æ–°è¿è¡Œä¸€æ¬¡ç”Ÿæˆå™¨æ¥ç¼“å­˜å‡†ç¡®çš„ç‰¹å¾
            with torch.no_grad():
                self.generator(
                    noisy_image_or_video=denoised_pred, 
                    timestep=0,
                    kv_cache=self.kv_cache1,  # æ›´æ–° Cache
                    current_start=current_start_frame * 1560
                )
            
            current_start_frame += 3  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª Block
            
        return output, timestep_from, timestep_to
```

### 6.3 DMD æŸå¤±è®¡ç®— (`model/dmd.py`)

```python
def compute_distribution_matching_loss(self, image_or_video, conditional_dict, ...):
    """
    DMD (Distribution Matching Distillation) æŸå¤±
    ç›®æ ‡: æœ€å°åŒ–ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒä¹‹é—´çš„ KL æ•£åº¦
    """
    # Step 1: å¯¹ç”Ÿæˆçš„è§†é¢‘æ·»åŠ éšæœºå™ªå£°
    timestep = self._get_timestep(...)
    noise = torch.randn_like(image_or_video)
    noisy_latent = scheduler.add_noise(image_or_video, noise, timestep)
    
    # Step 2: è®¡ç®— KL æ¢¯åº¦ (DMD è®ºæ–‡æ ¸å¿ƒ)
    with torch.no_grad():
        # Eq 7: grad = (Score_fake - Score_real)
        # Score_fake: Critic æ¨¡å‹çš„é¢„æµ‹
        # Score_real: é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹(Wan2.1-14B)çš„é¢„æµ‹
        grad, log_dict = self._compute_kl_grad(
            noisy_image_or_video=noisy_latent,
            estimated_clean_image_or_video=image_or_video,
            timestep=timestep,
            ...
        )
    
    # Step 3: è®¡ç®—æœ€ç»ˆ MSE æŸå¤±
    # ä¼˜åŒ–ç”Ÿæˆå™¨ï¼Œä½¿å…¶ç”Ÿæˆçš„æ ·æœ¬æœå‘ "çœŸå®åˆ†å¸ƒ" ç§»åŠ¨
    dmd_loss = 0.5 * F.mse_loss(
        image_or_video.double(),
        (image_or_video.double() - grad.double()).detach()
    )
    
    return dmd_loss, log_dict
```

---

## 7. å…³é”®ç»„ä»¶è¯¦è§£

### æ¨¡å‹å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           DMD æ¨¡å‹                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Generator     â”‚     â”‚   Real Score    â”‚     â”‚  Fake Score   â”‚ â”‚
â”‚  â”‚  (å­¦ç”Ÿæ¨¡å‹)      â”‚     â”‚   (æ•™å¸ˆæ¨¡å‹)     â”‚     â”‚  (Critic)     â”‚ â”‚
â”‚  â”‚                 â”‚     â”‚                 â”‚     â”‚               â”‚ â”‚
â”‚  â”‚ â€¢ 4æ­¥å»å™ª       â”‚     â”‚ â€¢ é¢„è®­ç»ƒæƒé‡     â”‚     â”‚ â€¢ å­¦ä¹ ç”Ÿæˆå™¨   â”‚ â”‚
â”‚  â”‚ â€¢ å› æœæ³¨æ„åŠ›    â”‚     â”‚ â€¢ Wan2.1-14B    â”‚     â”‚   çš„åˆ†å¸ƒ      â”‚ â”‚
â”‚  â”‚ â€¢ KV Cache     â”‚     â”‚ â€¢ å†»ç»“ä¸è®­ç»ƒ     â”‚     â”‚ â€¢ éœ€è¦è®­ç»ƒ    â”‚ â”‚
â”‚  â”‚ â€¢ éœ€è¦è®­ç»ƒ      â”‚     â”‚                 â”‚     â”‚               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                       â”‚                       â”‚         â”‚
â”‚           â–¼                       â–¼                       â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     WanDiffusionWrapper                      â”‚   â”‚
â”‚  â”‚  â€¢ CausalWanModel (æ ¸å¿ƒ DiT ç½‘ç»œ)                             â”‚   â”‚
â”‚  â”‚  â€¢ FlowMatchScheduler (æµåŒ¹é…è°ƒåº¦å™¨)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸ºä»€ä¹ˆå« Self-Forcing?

1.  **ä¼ ç»Ÿè®­ç»ƒ**ä½¿ç”¨ Ground Truth è§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œæ¨¡å‹å­¦ä¹ å»å™ªã€‚ä½†è¿™ä¼šå¯¼è‡´ **Train-Test Gap**ï¼ˆè®­ç»ƒæ—¶æœ‰å®Œç¾è¾“å…¥ï¼Œæ¨ç†æ—¶åªæœ‰é¢„æµ‹è¾“å…¥ï¼‰ã€‚
2.  **Self-Forcing** åœ¨è®­ç»ƒé˜¶æ®µç›´æ¥ä½¿ç”¨æ¨¡å‹è‡ªå·±ç”Ÿæˆçš„ï¼ˆå¯èƒ½æœ‰ç‘•ç–µçš„ï¼‰è¾“å‡ºä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ã€‚
3.  è¿™è¿«ä½¿æ¨¡å‹å­¦ä¼šä»è‡ªå·±çš„é”™è¯¯ä¸­æ¢å¤ï¼Œæå¤§åœ°å¢å¼ºäº†è‡ªå›å½’ç”Ÿæˆçš„ç¨³å®šæ€§ã€‚

### æ€»ç»“

è¿™ä»½ä»£ç å®ç°äº†ä¸€ä¸ªé«˜åº¦å¤æ‚çš„ **Data-Free** è§†é¢‘ç”Ÿæˆè’¸é¦ç³»ç»Ÿã€‚å®ƒä¸éœ€è¦ä»»ä½•è§†é¢‘æ•°æ®ï¼Œä»…ä¾é æ–‡æœ¬ Prompt å’Œä¸€ä¸ªå¼ºå¤§çš„é¢„è®­ç»ƒæ•™å¸ˆæ¨¡å‹ï¼Œå°±èƒ½è®­ç»ƒå‡ºä¸€ä¸ªèƒ½å¤Ÿå®æ—¶ç”Ÿæˆé«˜è´¨é‡é•¿è§†é¢‘çš„å­¦ç”Ÿæ¨¡å‹ï¼Œå¹¶ä¸”å·§å¦™åœ°è§£å†³äº†é•¿è§†é¢‘ç”Ÿæˆä¸­çš„ä¸€è‡´æ€§é—®é¢˜ã€‚
