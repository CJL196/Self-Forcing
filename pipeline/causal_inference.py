from typing import List, Optional
import torch

from utils.wan_wrapper import WanDiffusionWrapper, WanTextEncoder, WanVAEWrapper

from demo_utils.memory import gpu, get_cuda_free_memory_gb, DynamicSwapInstaller, move_model_to_device_with_memory_preservation


class CausalInferencePipeline(torch.nn.Module):
    def __init__(
            self,
            args,
            device,
            generator=None,
            text_encoder=None,
            vae=None
    ):
        super().__init__()
        # =================================================================================
        # 1. æ ¸å¿ƒæ¨¡å‹åˆå§‹åŒ– (Initialize all models)
        # =================================================================================
        # WanDiffusionWrapper: æ‰©æ•£ç”Ÿæˆå™¨æ ¸å¿ƒã€‚
        # è¿™é‡Œä¼ å…¥ is_causal=True æ˜¯è‡³å…³é‡è¦çš„ä¸€æ­¥ï¼
        # è¿™å‘Šè¯‰åº•å±‚æ¨¡å‹ï¼š"å˜¿ï¼Œæˆ‘ä»¬ç°åœ¨è¦åšè§†é¢‘ç”Ÿæˆäº†ï¼Œè¯·ä½ å¼€å¯ KV Caching æ¨¡å¼ï¼Œ
        # ä¸è¦åƒè®­ç»ƒæ—¶é‚£æ ·ä¸€æ¬¡æ€§çœ‹å…¨å›¾ï¼Œè€Œæ˜¯è¦åƒ GPT é‚£æ ·ä¸€æ®µä¸€æ®µåœ°ç”Ÿæˆã€‚"
        self.generator = WanDiffusionWrapper(
            **getattr(args, "model_kwargs", {}), is_causal=True) if generator is None else generator
        
        # WanTextEncoder: æ–‡æœ¬ç¼–ç å™¨ (é€šå¸¸æ˜¯ T5)ã€‚
        # è´Ÿè´£æŠŠç”¨æˆ·çš„ "A cat walking on the grass" å˜æˆæœºå™¨èƒ½æ‡‚çš„å‘é‡ (Embeddings)ã€‚
        self.text_encoder = WanTextEncoder() if text_encoder is None else text_encoder
        
        # WanVAEWrapper: å˜åˆ†è‡ªç¼–ç å™¨ã€‚
        # æ‰©æ•£æ¨¡å‹æ˜¯åœ¨â€œæ½œç©ºé—´ (Latent Space)â€é‡Œå·¥ä½œçš„ (å‹ç¼©åçš„æ¨¡ç³Šç‰¹å¾)ã€‚
        # VAE è´Ÿè´£æœ€åä¸€æ­¥ï¼šæŠŠæ½œç©ºé—´çš„ç‰¹å¾â€œè§£å‹â€å›äººçœ¼èƒ½çœ‹çš„åƒç´ è§†é¢‘ã€‚
        self.vae = WanVAEWrapper() if vae is None else vae

        # =================================================================================
        # 2. åˆå§‹åŒ–å› æœæ¨ç†è¶…å‚æ•° (Initialize all causal hyperparmeters)
        # =================================================================================
        self.scheduler = self.generator.get_scheduler()
        
        # denoising_step_list: å»å™ªæ­¥æ•°è®¡åˆ’è¡¨ã€‚
        # åœ¨ Self-Forcing ç®—æ³•ä¸­ï¼Œå› ä¸ºä½¿ç”¨äº† DMD è’¸é¦æŠ€æœ¯ï¼Œæ­¥æ•°é€šå¸¸éå¸¸å°‘ (ä¾‹å¦‚åªæœ‰ 4 æ­¥)ã€‚
        # æ¯”å¦‚: [1000, 750, 500, 250]ã€‚è¿™æ„å‘³ç€æ¯ç”Ÿæˆä¸€å°æ®µè§†é¢‘ï¼Œåªéœ€è¦æ¨¡å‹è·‘ 4 æ¬¡ã€‚
        self.denoising_step_list = torch.tensor(
            args.denoising_step_list, dtype=torch.long)
        
        # warp_denoising_step: ä¸€ä¸ªé«˜çº§çš„æ—¶é—´æ­¥æ˜ å°„æŠ€å·§ï¼Œç”¨äºå¾®è°ƒé‡‡æ ·è¿‡ç¨‹ã€‚
        if args.warp_denoising_step:
            timesteps = torch.cat((self.scheduler.timesteps.cpu(), torch.tensor([0], dtype=torch.float32)))
            self.denoising_step_list = timesteps[1000 - self.denoising_step_list]

        # Wan2.1 æ¨¡å‹çš„æ ‡å‡†é…ç½®
        self.num_transformer_blocks = 30  # æ¨¡å‹æ·±åº¦
        self.frame_seq_length = 1560      # æ¯å¸§å¯¹åº”çš„ Token æ•°é‡ (ç»è¿‡ Patchify å)

        # KV Cache å®¹å™¨ï¼Œç¨ååœ¨ inference ä¸­ä¼šåˆ†é…å·¨å¤§çš„æ˜¾å­˜ç»™å®ƒ
        self.kv_cache1 = None
        self.args = args
        
        # num_frame_per_block: ã€å…³é”®å‚æ•°ã€‘
        # æ§åˆ¶æ¯æ¬¡ç”Ÿæˆå¤šå°‘å¸§ã€‚åœ¨ Self-Forcing ä¸­ï¼Œé€šå¸¸è®¾ä¸º 3ã€‚
        # è¿™æ„å‘³ç€ï¼šç”Ÿæˆ 0-3 å¸§ -> å›ºåŒ– -> ç”Ÿæˆ 3-6 å¸§ (çœ‹ 0-3) -> å›ºåŒ– -> ç”Ÿæˆ 6-9 å¸§...
        self.num_frame_per_block = getattr(args, "num_frame_per_block", 1)
        self.independent_first_frame = args.independent_first_frame
        self.local_attn_size = self.generator.model.local_attn_size

        print(f"KV inference with {self.num_frame_per_block} frames per block (æ¯æ¬¡ç”Ÿæˆå¸§æ•°)")

        if self.num_frame_per_block > 1:
            self.generator.model.num_frame_per_block = self.num_frame_per_block

    def inference(
        self,
        noise: torch.Tensor,
        text_prompts: List[str],
        initial_latent: Optional[torch.Tensor] = None,
        return_latents: bool = False,
        profile: bool = False,
        low_memory: bool = False,
    ) -> torch.Tensor:
        """
        æ ¸å¿ƒæ¨ç†å‡½æ•°ï¼šæ‰§è¡Œ Self-Forcing çš„å› æœæ¨ç†è¿‡ç¨‹ã€‚

        âš ï¸ å®è§‚é€»è¾‘ (Block-Based Autoregressive Generation):
        1. åˆ†å— (Split): æˆ‘ä»¬ä¸ä¼šä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰å¸§ï¼Œè€Œæ˜¯æŠŠè§†é¢‘åˆ‡æˆå°å—(Block), æ¯ä¸ª Block æ¯”å¦‚ 3 å¸§ã€‚
        2. æ¥é¾™ (Chain): å…ˆç”Ÿæˆç¬¬ 1 å—ã€‚ç”Ÿæˆå¥½åï¼ŒæŠŠå®ƒçš„ç‰¹å¾"å†»ç»“"å¹¶å­˜å…¥ KV Cacheã€‚
        3. ä¾èµ– (dependency): ç”Ÿæˆç¬¬ 2 å—æ—¶ï¼Œæ¨¡å‹ä¼šè¯»å– Cache é‡Œçš„ç¬¬ 1 å—ç‰¹å¾ï¼Œç¡®ä¿è¿è´¯æ€§ã€‚
        4. å¾ªç¯ (Loop): å¦‚æ­¤å¾€å¤ï¼Œç›´åˆ°ç”Ÿæˆå®Œæ•´è§†é¢‘ã€‚

        å‚æ•°è§£æ:
        - noise: [Batch, Total_Frames, C, H, W]ã€‚
            è¿™æ˜¯é«˜æ–¯å™ªå£°èµ·ç‚¹ã€‚æ³¨æ„ï¼šå®ƒçš„å½¢çŠ¶(Total_Frames)ç›´æ¥å†³å®šäº†æœ€ç»ˆç”Ÿæˆçš„è§†é¢‘é•¿åº¦ã€‚
        - text_prompts: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æç¤ºè¯ã€‚
        - initial_latent: [Batch, Input_Frames, C, H, W]ã€‚
            I2V (å›¾ç”Ÿè§†é¢‘) æ—¶ï¼Œè¿™é‡Œæ˜¯é¦–å¸§çš„ Latentã€‚
            Video Extension (è§†é¢‘æ‰©å……) æ—¶ï¼Œè¿™é‡Œæ˜¯å‰æ®µè§†é¢‘çš„ Latentã€‚
            T2V (æ–‡ç”Ÿè§†é¢‘) æ—¶ï¼Œè¿™é‡Œæ˜¯ Noneã€‚
        """
        batch_size, num_frames, num_channels, height, width = noise.shape
        
        # =================================================================================
        # 2.2 å˜é‡åˆå§‹åŒ–ä¸åˆ†å—è®¡ç®—
        # è®¡ç®—æˆ‘ä»¬éœ€è¦å¾ªç¯å¤šå°‘æ¬¡ (num_blocks) æ‰èƒ½å¡«æ»¡è¿™ num_frames å¸§
        # =================================================================================
        if not self.independent_first_frame or (self.independent_first_frame and initial_latent is not None):
            # è¿™æ˜¯æœ€å¸¸è§çš„æƒ…å†µã€‚ä¾‹å¦‚ï¼š
            # å¦‚æœæˆ‘ä»¬è¦ç”Ÿæˆ 21 å¸§ï¼Œæ¯å— 3 å¸§ (num_frame_per_block=3)ã€‚
            # é‚£ä¹ˆ num_blocks = 21 / 3 = 7ã€‚æˆ‘ä»¬éœ€è¦è·‘ 7 æ¬¡å¤§å¾ªç¯ã€‚
            assert num_frames % self.num_frame_per_block == 0
            num_blocks = num_frames // self.num_frame_per_block
        else:
            # è¿™æ˜¯ä¸€ä¸ªæå°‘ç”¨çš„æµ‹è¯•åˆ†æ”¯ï¼Œç¬¬ä¸€å¸§ç‹¬ç«‹ç”Ÿæˆï¼Œä¸ç”¨ç®¡å®ƒ
            assert (num_frames - 1) % self.num_frame_per_block == 0
            num_blocks = (num_frames - 1) // self.num_frame_per_block
        num_input_frames = initial_latent.shape[1] if initial_latent is not None else 0
        num_output_frames = num_frames + num_input_frames  # add the initial latent frames
        
        # 2.3 æ–‡æœ¬ç¼–ç 
        # è°ƒç”¨ T5 Encoderï¼ŒæŠŠ Prompt æ–‡æœ¬å˜æˆ embeddingsã€‚
        # è¿™äº› embeddings ä¼šè¢«åç»­æ‰€æœ‰å¸§çš„ç”Ÿæˆè¿‡ç¨‹å¤ç”¨ã€‚
        conditional_dict = self.text_encoder(
            text_prompts=text_prompts
        )

        if low_memory:
            gpu_memory_preservation = get_cuda_free_memory_gb(gpu) + 5
            move_model_to_device_with_memory_preservation(self.text_encoder, target_device=gpu, preserved_memory_gb=gpu_memory_preservation)

        # 2.4 å‡†å¤‡è¾“å‡ºç”»å¸ƒ
        # åˆ›å»ºä¸€ä¸ªå…¨é›¶å¼ é‡ (Canvas)ã€‚
        # æ­¤æ—¶å®ƒé‡Œé¢ä»€ä¹ˆéƒ½æ²¡æœ‰ï¼Œæˆ‘ä»¬æ¥ä¸‹æ¥çš„å¾ªç¯ä¼šæŠŠç”Ÿæˆå¥½çš„ latents ä¸€å—ä¸€å—â€œå¡«â€è¿›å»ã€‚
        output = torch.zeros(
            [batch_size, num_output_frames, num_channels, height, width],
            device=noise.device,
            dtype=noise.dtype
        )

        # Set up profiling if requested
        if profile:
            init_start = torch.cuda.Event(enable_timing=True)
            init_end = torch.cuda.Event(enable_timing=True)
            diffusion_start = torch.cuda.Event(enable_timing=True)
            diffusion_end = torch.cuda.Event(enable_timing=True)
            vae_start = torch.cuda.Event(enable_timing=True)
            vae_end = torch.cuda.Event(enable_timing=True)
            block_times = []
            block_start = torch.cuda.Event(enable_timing=True)
            block_end = torch.cuda.Event(enable_timing=True)
            init_start.record()

        # =================================================================================
        # ğŸ”¥ Step 1: KV Cache çš„åˆå§‹åŒ– (Infrastructure Setup)
        # =================================================================================
        # è¿™æ˜¯å› æœæ¨ç†çš„åœ°åŸºã€‚
        if self.kv_cache1 is None:
            # === Case A: ç¬¬ä¸€æ¬¡è¿è¡Œ ===
            # æˆ‘ä»¬éœ€è¦å‘ GPU ç”³è¯·ä¸€å¤§å—æ˜¾å­˜ã€‚
            # kv_cache1 æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ç­‰äº Transformer å±‚æ•° (30)ã€‚
            # æ¯ä¸€å±‚åŒ…å«ä¸€ä¸ªå­—å…¸ï¼š{'k': ..., 'v': ..., 'global_end_index': ...}
            # æ³¨æ„ï¼š_initialize_kv_cache ä¼šæŒ‰ç…§ã€æœ€å¤§å¯èƒ½çš„åºåˆ—é•¿åº¦ã€‘ä¸€æ¬¡æ€§åˆ†é…å†…å­˜ï¼Œ
            # è€Œä¸æ˜¯åŠ¨æ€ appendã€‚è¿™èƒ½æå¤§åœ°å‡å°‘æ˜¾å­˜ç¢ç‰‡ã€‚
            self._initialize_kv_cache(
                batch_size=batch_size,
                dtype=noise.dtype,
                device=noise.device
            )
            # åˆå§‹åŒ– Cross-Attention Cache (ç”¨äºç¼“å­˜æ–‡æœ¬ç‰¹å¾çš„ attention ç»“æœ)
            self._initialize_crossattn_cache(
                batch_size=batch_size,
                dtype=noise.dtype,
                device=noise.device
            )
        else:
            # === Case B: æ˜¾å­˜å¤ç”¨ (Memory Reuse) ===
            # å¦‚æœä¹‹å‰çš„æ¨ç†å·²ç»åˆ†é…è¿‡ cacheï¼Œæˆ‘ä»¬ç›´æ¥å¤ç”¨ç‰©ç†æ˜¾å­˜ã€‚
            # åªæ˜¯æŠŠ index æŒ‡é’ˆå½’é›¶ã€‚è¿™æ¯” free å† malloc å¿«å¾—å¤šï¼Œä¸”å®‰å…¨ã€‚
            
            # é‡ç½® Cross Attention çŠ¶æ€
            for block_index in range(self.num_transformer_blocks):
                self.crossattn_cache[block_index]["is_init"] = False
            
            # é‡ç½® KV Cache æŒ‡é’ˆ
            for block_index in range(len(self.kv_cache1)):
                # global_end_index = 0 æ„å‘³ç€æˆ‘ä»¬é€»è¾‘ä¸Šæ¸…ç©ºäº† cacheï¼Œ
                # ä½†ç‰©ç†æ˜¾å­˜è¿˜åœ¨é‚£é‡Œï¼Œç­‰å¾…è¢«æ–°æ•°æ®è¦†ç›–ã€‚
                self.kv_cache1[block_index]["global_end_index"] = torch.tensor(
                    [0], dtype=torch.long, device=noise.device)
                self.kv_cache1[block_index]["local_end_index"] = torch.tensor(
                    [0], dtype=torch.long, device=noise.device)

        # =================================================================================
        # ğŸ”¥ Step 2: é¢„å¡«å……ä¸Šä¸‹æ–‡ (Prefill Context)
        # =================================================================================
        # åœºæ™¯ï¼šI2V (å›¾ç”Ÿè§†é¢‘) æˆ– è§†é¢‘ç»­å†™ã€‚
        # é—®é¢˜ï¼šKV Cache ç°åœ¨æ˜¯ç©ºçš„ã€‚å¦‚æœç›´æ¥å¼€å§‹ç”Ÿæˆï¼Œæ¨¡å‹ä¸çŸ¥é“å‰é¢çš„å†å²ä¿¡æ¯ã€‚
        # è§£å†³ï¼šæˆ‘ä»¬éœ€è¦æŠŠå·²çŸ¥çš„å†å²å¸§ (initial_latent) å…ˆâ€œè¿‡ä¸€éâ€æ¨¡å‹ï¼Œå­˜å…¥ Cacheã€‚

        current_start_frame = 0
        if initial_latent is not None:
            # è®¾ç½® timestep ä¸º 0ã€‚
            # åœ¨ Diffusion ä¸­ï¼Œt=0 æ„å‘³ç€â€œæ²¡æœ‰å™ªå£°â€ï¼Œå³æ¸…æ™°å›¾åƒã€‚
            # æˆ‘ä»¬å‘Šè¯‰æ¨¡å‹ï¼šâ€œå˜¿ï¼Œè¿™æ˜¯å®Œç¾çš„å†å²æ•°æ®ï¼Œè¯·è®°ä½å®ƒã€‚â€ (Teacher Forcing)
            timestep = torch.ones([batch_size, 1], device=noise.device, dtype=torch.int64) * 0
            
            if self.independent_first_frame:
                 # (å¤„ç†ç¬¬ä¸€å¸§ç‹¬ç«‹çš„ç‰¹æ®Šé€»è¾‘)
                # Assume num_input_frames is 1 + self.num_frame_per_block * num_input_blocks
                assert (num_input_frames - 1) % self.num_frame_per_block == 0
                num_input_blocks = (num_input_frames - 1) // self.num_frame_per_block
                output[:, :1] = initial_latent[:, :1]
                self.generator(
                    noisy_image_or_video=initial_latent[:, :1],
                    conditional_dict=conditional_dict,
                    timestep=timestep * 0,
                    kv_cache=self.kv_cache1,
                    crossattn_cache=self.crossattn_cache,
                    current_start=current_start_frame * self.frame_seq_length,
                )
                current_start_frame += 1
            else:
                # æ­£å¸¸é€»è¾‘ï¼šè®¡ç®—æœ‰å¤šå°‘ä¸ªå†å²å—éœ€è¦é¢„å¡«
                assert num_input_frames % self.num_frame_per_block == 0
                num_input_blocks = num_input_frames // self.num_frame_per_block

            # éå†æ¯ä¸€ä¸ªå†å²å—
            for _ in range(num_input_blocks):
                # åˆ‡ç‰‡ï¼šå–å‡ºå½“å‰è¿™å‡ å¸§å†å²æ•°æ®
                current_ref_latents = \
                    initial_latent[:, current_start_frame:current_start_frame + self.num_frame_per_block]
                
                # å¡«å…¥ output ç”»å¸ƒ (è™½ç„¶æ˜¯è¾“å…¥ï¼Œä½†ä¹Ÿæ”¾åœ¨ output é‡Œä¿æŒå®Œæ•´)
                output[:, current_start_frame:current_start_frame + self.num_frame_per_block] = current_ref_latents
                
                # === å…³é”®åŠ¨ä½œ (Key Action) ===
                # è¿è¡Œ Generatorã€‚æ³¨æ„ï¼è¿™é‡Œæˆ‘ä»¬ã€ä¸æ¥æ”¶è¿”å›å€¼ã€‘ï¼
                # æˆ‘ä»¬å®Œå…¨ä¸åœ¨ä¹å®ƒçš„è¾“å‡ºæ˜¯ä»€ä¹ˆã€‚
                # æˆ‘ä»¬åªåœ¨ä¹å®ƒçš„ã€å‰¯ä½œç”¨ã€‘ï¼šæ›´æ–° self.kv_cache1ã€‚
                # å®ƒä¼šè®¡ç®— current_ref_latents çš„ K, V å¹¶è¿½åŠ å†™å…¥ç¼“å­˜ã€‚
                self.generator(
                    noisy_image_or_video=current_ref_latents,
                    conditional_dict=conditional_dict,
                    timestep=timestep * 0, # t=0, å¼ºåˆ¶ Teacher Forcing
                    kv_cache=self.kv_cache1, # ä¼ å…¥ Cache å¯¹è±¡ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨å†™å…¥
                    crossattn_cache=self.crossattn_cache,
                    current_start=current_start_frame * self.frame_seq_length,
                )
                
                # ç§»åŠ¨æŒ‡é’ˆï¼Œå¤„ç†ä¸‹ä¸€ä¸ªå—
                current_start_frame += self.num_frame_per_block

        if profile:
            init_end.record()
            torch.cuda.synchronize()
            diffusion_start.record()

        # =================================================================================
        # ğŸ”¥ğŸ”¥ Step 3: æ ¸å¿ƒæ—¶åºå»å™ªå¾ªç¯ (Temporal Denoising Loop)
        # =================================================================================
        # è¿™æ˜¯æ•´ä¸ªæ¨ç†è¿‡ç¨‹çš„å¿ƒè„ï¼Œå®ç°äº† Self-Forcing æœºåˆ¶ã€‚
        # å‡†å¤‡ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯”å¦‚ [3, 3, 3, 3, 3, 3, 3]ï¼Œè¡¨ç¤ºæ¯ä¸ª Block åŒ…å«çš„å¸§æ•°
        all_num_frames = [self.num_frame_per_block] * num_blocks
        if self.independent_first_frame and initial_latent is None:
            all_num_frames = [1] + all_num_frames
            
        # === å¤–å±‚å¾ªç¯ï¼šéå†æ¯ä¸€ä¸ª Block (æŒ‰æ—¶é—´é¡ºåº) ===
        for current_num_frames in all_num_frames:
            if profile:
                block_start.record()

            # 1. å‡†å¤‡å™ªå£° inputs
            # ä»åŸå§‹çš„å¤§ noise tensor ä¸­åˆ‡å‡ºå½“å‰è¿™å‡ å¸§çš„å™ªå£°
            noisy_input = noise[
                :, current_start_frame - num_input_frames:current_start_frame + current_num_frames - num_input_frames]

            # === Step 3.1: å†…å±‚å¾ªç¯ï¼šç©ºé—´å»å™ª (Spatial Denoising) ===
            # åœ¨å½“å‰è¿™ä¸ªæ—¶é—´å—å†…ï¼Œä»çº¯å™ªå£°é€æ­¥è¿˜åŸå‡ºå›¾åƒã€‚
            # denoising_step_list å¯èƒ½æ˜¯ [1000, 750, 500, 250]
            for index, current_timestep in enumerate(self.denoising_step_list):
                print(f"current_timestep: {current_timestep}")
                
                # æ„é€  timestep å¼ é‡
                timestep = torch.ones(
                    [batch_size, current_num_frames],
                    device=noise.device,
                    dtype=torch.int64) * current_timestep

                if index < len(self.denoising_step_list) - 1:
                    # Case 1: è¿˜æ²¡åˆ°æœ€åä¸€æ­¥
                    # è¿è¡Œç”Ÿæˆå™¨é¢„æµ‹å»å™ªç»“æœ (denoised_pred)
                    # ã€é‡è¦ã€‘è¿™é‡Œä¼ å…¥äº† kv_cache1ï¼Œå› ä¸ºæˆ‘ä»¬è¦è¯»å–ä¹‹å‰çš„å†å²ä¿¡æ¯ï¼
                    # ä½†æ˜¯ï¼Œå› ä¸º timestep > 0 (è¿˜æ²¡å½»åº•å¹²å‡€)ï¼Œä¸»è¦ç›®çš„æ˜¯åˆ©ç”¨ Cacheï¼Œ
                    # æ­¤æ—¶æ¨¡å‹ã€ä¸ä¼šã€‘æŠŠå½“å‰è¿™å‡ å¸§å†™å…¥ Cacheã€‚
                    _, denoised_pred = self.generator(
                        noisy_image_or_video=noisy_input,
                        conditional_dict=conditional_dict,
                        timestep=timestep,
                        kv_cache=self.kv_cache1,
                        crossattn_cache=self.crossattn_cache,
                        current_start=current_start_frame * self.frame_seq_length
                    )
                    
                    # è°ƒåº¦å™¨æ›´æ–° (Step): åŠ ç‚¹å™ªå£°å‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
                    # ç±»ä¼¼äº x_{t-1} = x_0 + noise
                    next_timestep = self.denoising_step_list[index + 1]
                    noisy_input = self.scheduler.add_noise(
                        denoised_pred.flatten(0, 1),
                        torch.randn_like(denoised_pred.flatten(0, 1)),
                        next_timestep * torch.ones(
                            [batch_size * current_num_frames], device=noise.device, dtype=torch.long)
                    ).unflatten(0, denoised_pred.shape[:2])
                else:
                    # Case 2: æœ€åä¸€æ­¥
                    # denoised_pred å°±æ˜¯æˆ‘ä»¬ç»ˆäºç”Ÿæˆå¥½çš„ clean latents
                    _, denoised_pred = self.generator(
                        noisy_image_or_video=noisy_input,
                        conditional_dict=conditional_dict,
                        timestep=timestep,
                        kv_cache=self.kv_cache1,
                        crossattn_cache=self.crossattn_cache,
                        current_start=current_start_frame * self.frame_seq_length
                    )

            # Step 3.2: è®°å½•ç»“æœ
            # åˆ°è¿™é‡Œï¼Œå½“å‰ Block (3å¸§) å·²ç»å®Œå…¨ç”Ÿæˆå®Œæ¯•äº†ï¼ä¿å­˜å®ƒã€‚
            output[:, current_start_frame:current_start_frame + current_num_frames] = denoised_pred

            # =================================================================================
            # ğŸ”¥ Step 3.3: Self-Forcing æ›´æ–° (Critial Step)
            # =================================================================================
            # æ­¤æ—¶ï¼ŒKV Cache é‡Œè¿˜æ²¡æœ‰è¿™ 3 å¸§çš„ä¿¡æ¯ï¼ˆä¹‹å‰åªæ˜¯åœ¨ Readï¼Œæ²¡ Writeï¼‰ã€‚
            # ä¸ºäº†è®©ä¸‹ä¸€ä¸ª Block èƒ½å‚è€ƒè¿™ 3 å¸§ï¼Œæˆ‘ä»¬å¿…é¡»åšä¸€æ¬¡é¢å¤–çš„è½¬å‘æ¥â€œè®°å½•â€å®ƒä»¬ã€‚
            
            # 1. æ„é€ ä¸€ä¸ªæå°çš„ timestep (é€šå¸¸æ˜¯ args.context_noise æˆ– 0)
            context_timestep = torch.ones_like(timestep) * self.args.context_noise
            
            # 2. å†è·‘ä¸€æ¬¡æ¨¡å‹ï¼è¿™æ˜¯ä¸€ä¸ªé¢å¤–çš„ overheadï¼Œä½†å¿…ä¸å¯å°‘ã€‚
            # è¿™æ¬¡è¾“å…¥çš„æ˜¯åˆšåˆšç”Ÿæˆçš„å®Œç¾ç»“æœ (denoised_pred)ã€‚
            self.generator(
                noisy_image_or_video=denoised_pred,
                conditional_dict=conditional_dict,
                timestep=context_timestep,
                kv_cache=self.kv_cache1,
                crossattn_cache=self.crossattn_cache,
                current_start=current_start_frame * self.frame_seq_length,
                # ã€æœºåˆ¶è¯¦è§£ã€‘
                # å½“ Generator å‘ç° timestep â‰ˆ 0 æ—¶ï¼Œå®ƒä¼šå¯åŠ¨ update logicã€‚
                # å®ƒä¼šè®¡ç®—å½“å‰è¿™ 3 å¸§çš„ Key å’Œ Valueï¼Œå¹¶å°†å…¶ boost (è¿½åŠ ) åˆ° kv_cache1 çš„æœ«å°¾ã€‚
                # è¿™æ ·ï¼Œä¸‹ä¸€æ¬¡å¤§å¾ªç¯ (Next Block) å°±èƒ½çœ‹åˆ°è¿™æ®µå†å²äº†ã€‚
            )

            if profile:
                block_end.record()
                torch.cuda.synchronize()
                block_time = block_start.elapsed_time(block_end)
                block_times.append(block_time)

            # Step 3.4: update the start and end frame indices
            current_start_frame += current_num_frames

        if profile:
            # End diffusion timing and synchronize CUDA
            diffusion_end.record()
            torch.cuda.synchronize()
            diffusion_time = diffusion_start.elapsed_time(diffusion_end)
            init_time = init_start.elapsed_time(init_end)
            vae_start.record()

        # =================================================================================
        # Step 4: è§£ç  (Video Decoding)
        # =================================================================================
        # æ­¤æ—¶ output åŒ…å«äº†æ‰€æœ‰ç”Ÿæˆçš„ Latent Framesã€‚æˆ‘ä»¬éœ€è¦ç”¨ VAE æŠŠå®ƒä»¬å˜å›äººçœ¼å¯çœ‹çš„åƒç´ ã€‚
        # use_cache=False: è¿™é‡Œä¸ä½¿ç”¨ VAE Cacheï¼Œç›´æ¥è§£ç ã€‚
        video = self.vae.decode_to_pixel(output, use_cache=False)
        
        # å½’ä¸€åŒ–: [-1, 1] -> [0, 1]
        video = (video * 0.5 + 0.5).clamp(0, 1)

        if profile:
            # End VAE timing and synchronize CUDA
            vae_end.record()
            torch.cuda.synchronize()
            vae_time = vae_start.elapsed_time(vae_end)
            total_time = init_time + diffusion_time + vae_time

            print("Profiling results:")
            print(f"  - Initialization/caching time: {init_time:.2f} ms ({100 * init_time / total_time:.2f}%)")
            print(f"  - Diffusion generation time: {diffusion_time:.2f} ms ({100 * diffusion_time / total_time:.2f}%)")
            for i, block_time in enumerate(block_times):
                print(f"    - Block {i} generation time: {block_time:.2f} ms ({100 * block_time / diffusion_time:.2f}% of diffusion)")
            print(f"  - VAE decoding time: {vae_time:.2f} ms ({100 * vae_time / total_time:.2f}%)")
            print(f"  - Total time: {total_time:.2f} ms")

        if return_latents:
            return video, output
        else:
            return video

    def _initialize_kv_cache(self, batch_size, dtype, device):
        """
        åˆå§‹åŒ– Wan æ¨¡å‹çš„ Per-GPU KV ç¼“å­˜ã€‚
        
        ç­–ç•¥ï¼šé™æ€é¢„åˆ†é… (Static Pre-allocation)ã€‚
        æˆ‘ä»¬ä¸ä½¿ç”¨ python list åŠ¨æ€ appendï¼Œå› ä¸ºé‚£ä¼šå¯¼è‡´å¤§é‡çš„æ˜¾å­˜ç¢ç‰‡ (Fragmentation)ã€‚
        ç›¸åï¼Œæˆ‘ä»¬æ ¹æ®æœ€å¤§å¯èƒ½çš„ Token æ•°é‡ï¼Œä¸€æ¬¡æ€§ç”³è¯·ä¸€ä¸ªå·¨å¤§çš„ Tensor çŸ©é˜µã€‚
        
        kv_cache_size = 32760 å¯¹åº”äº†çº¦ 21 å¸§ * 1560 tokens çš„å®¹é‡ã€‚
        """
        kv_cache1 = []
        if self.local_attn_size != -1:
            # kv_cache_size = æ»‘åŠ¨çª—å£å¸§æ•° * æ¯å¸§ Token æ•° (1560)
            # è¿™ç§æ¨¡å¼ä¸‹ Cache åƒå¾ªç¯é˜Ÿåˆ—ï¼Œåªå­˜å‚¨æœ€è¿‘ N å¸§çš„ä¿¡æ¯
            kv_cache_size = self.local_attn_size * self.frame_seq_length
        else:
            # 32760 = 21 å¸§ * 1560 Tokens/å¸§
            # 21 å¸§æ˜¯ Wan2.1 æ ‡å‡†çš„ç”Ÿæˆé•¿åº¦ï¼Œè¿™é‡Œä¸€æ¬¡æ€§é¢„åˆ†é…æ˜¾å­˜é˜²æ­¢ç¢ç‰‡
            kv_cache_size = 32760

        for _ in range(self.num_transformer_blocks):
            # å¼ é‡å½¢çŠ¶è§£é‡Š [batch_size, kv_cache_size, 12, 128]:
            # 1. batch_size: æ ·æœ¬æ•°é‡
            # 2. kv_cache_size: æœ€å¤§åºåˆ—é•¿åº¦ (æ€» Token å®¹é‡)
            # 3. 12: æ³¨æ„åŠ›å¤´æ•° (Wan2.1-1.3B è§„æ ¼)
            # 4. 128: æ¯ä¸ªå¤´çš„ç»´åº¦ (12 * 128 = 1536ï¼Œå³æ¨¡å‹çš„ä¸»éšè—å±‚ç»´åº¦)
            kv_cache1.append({
                "k": torch.zeros([batch_size, kv_cache_size, 12, 128], dtype=dtype, device=device),
                "v": torch.zeros([batch_size, kv_cache_size, 12, 128], dtype=dtype, device=device),
                "global_end_index": torch.tensor([0], dtype=torch.long, device=device),
                "local_end_index": torch.tensor([0], dtype=torch.long, device=device)
            })

        self.kv_cache1 = kv_cache1  # always store the clean cache

    def _initialize_crossattn_cache(self, batch_size, dtype, device):
        """
        åˆå§‹åŒ– Cross-Attention (æ–‡æœ¬-è§†é¢‘æ³¨æ„åŠ›) ç¼“å­˜ã€‚
        
        ç”¨é€”ï¼šç¼“å­˜æ–‡æœ¬ Prompt çš„ Attention ç»“æœã€‚
        åŸå› ï¼šå› ä¸ºæ–‡æœ¬ Prompt åœ¨æ•´ä¸ªè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ä¸­æ˜¯ä¸å˜çš„ (Temporal Invariant)ã€‚
             å¦‚æœæ²¡æœ‰è¿™ä¸ª Cacheï¼Œæ¯ä¸€å¸§ (æ¯ä¸ª Block) éƒ½è¦é‡æ–°è®¡ç®—ä¸€æ¬¡ Text-to-Image Attentionï¼Œ
             è¿™ä¼šæµªè´¹å¤§é‡çš„é‡å¤ç®—åŠ›ã€‚æœ‰äº†å®ƒï¼Œåªéœ€è¦è®¡ç®—ä¸€æ¬¡ã€‚
        """
        crossattn_cache = []

        for _ in range(self.num_transformer_blocks):
            # å¼ é‡å½¢çŠ¶è§£é‡Š [batch_size, 512, 12, 128]:
            # 1. batch_size: æ ·æœ¬æ•°é‡
            # 2. 512: æ–‡æœ¬æœ€å¤§ Token é•¿åº¦ (å¯¹åº” T5 Encoder çš„ seq_len)
            # 3. 12: æ³¨æ„åŠ›å¤´æ•°
            # 4. 128: æ¯ä¸ªå¤´çš„ç»´åº¦ (12 * 128 = 1536)
            crossattn_cache.append({
                "k": torch.zeros([batch_size, 512, 12, 128], dtype=dtype, device=device),
                "v": torch.zeros([batch_size, 512, 12, 128], dtype=dtype, device=device),
                "is_init": False
            })
        self.crossattn_cache = crossattn_cache
