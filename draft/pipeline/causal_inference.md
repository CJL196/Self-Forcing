# CausalInferencePipeline æºç è¯¦è§£

[pipeline/causal_inference.py](file:///home/node1/Desktop/n1/ai/videogen/Self-Forcing/pipeline/causal_inference.py)

è¯¥æ–‡ä»¶å®ç°äº† Self-Forcing çš„**å› æœæ¨ç†ç®¡çº¿**ï¼Œæ˜¯é¡¹ç›®ä¸­æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ã€‚å®ƒè´Ÿè´£åè°ƒæ–‡æœ¬ç¼–ç ã€æ½œåœ¨ç©ºé—´ç”Ÿæˆï¼ˆé€šè¿‡è‡ªå›å½’æ–¹å¼ï¼‰å’Œ VAE è§£ç ã€‚

---

## 1. å¯¼å…¥ä¸ç±»å®šä¹‰ (L1-46)

### å¯¼å…¥ä¾èµ–
```python
from typing import List, Optional
import torch

from utils.wan_wrapper import WanDiffusionWrapper, WanTextEncoder, WanVAEWrapper
from demo_utils.memory import gpu, get_cuda_free_memory_gb, DynamicSwapInstaller, move_model_to_device_with_memory_preservation
```
- `WanDiffusionWrapper`: æ‰©æ•£ç”Ÿæˆå™¨
- `WanTextEncoder`: æ–‡æœ¬ç¼–ç å™¨
- `WanVAEWrapper`: å˜åˆ†è‡ªç¼–ç å™¨
- `DynamicSwapInstaller`: åŠ¨æ€æ˜¾å­˜ç®¡ç†ï¼ˆç”¨äºä½æ˜¾å­˜æ¨¡å¼ï¼‰

### åˆå§‹åŒ–å‡½æ•° `__init__`

```python
class CausalInferencePipeline(torch.nn.Module):
    def __init__(self, args, device, generator=None, text_encoder=None, vae=None):
        super().__init__()
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.generator = WanDiffusionWrapper(..., is_causal=True)
        self.text_encoder = WanTextEncoder()
        self.vae = WanVAEWrapper()

        # 2. åˆå§‹åŒ–è¶…å‚æ•°
        self.scheduler = self.generator.get_scheduler()
        self.denoising_step_list = torch.tensor(args.denoising_step_list, dtype=torch.long)
        # å¸¸è§ step_list: [1000, 750, 500, 250] (4æ­¥æ¨ç†)

        # 3. å¯¹åº” Wan2.1 çš„é…ç½®
        self.num_transformer_blocks = 30
        self.frame_seq_length = 1560  # æ¯å¸§ 1560 tokens
        self.kv_cache1 = None         # KV ç¼“å­˜å®¹å™¨
        self.num_frame_per_block = getattr(args, "num_frame_per_block", 1)  # é»˜è®¤ 3 å¸§ä¸€å—
```

**å…³é”®ç‚¹**ï¼š
- `is_causal=True`: å¯ç”¨å› æœæ¨¡å¼ï¼Œæ”¯æŒ KV ç¼“å­˜ã€‚
- `num_frame_per_block`: æ§åˆ¶æ¯æ¬¡ç”Ÿæˆå¤šå°‘å¸§ï¼ˆSelf-Forcing é»˜è®¤ä¸º 3ï¼‰ã€‚
- `denoising_step_list`: åªæœ‰ 4 æ­¥ï¼Œå› ä¸ºæ¨¡å‹ç»è¿‡äº† DMD è’¸é¦ã€‚

---

## 2. æ ¸å¿ƒæ¨ç†å‡½æ•° `inference` è¶…çº§æ·±åº¦è§£æ (L47-276)

âš ï¸ **æ³¨æ„**ï¼šæœ¬ç« èŠ‚å°†è¿›è¡Œä¿å§†çº§çš„è¯¦ç»†æ‹†è§£ï¼Œç¡®ä¿æ¯ä¸€ä¸ªå˜é‡ã€æ¯ä¸€ä¸ªå¾ªç¯ã€æ¯ä¸€ä¸ªæ¡ä»¶çš„æ„å›¾éƒ½è§£é‡Šå¾—æ¸…æ¸…æ¥šæ¥šã€‚

### 2.0 å®è§‚é€»è¾‘

åœ¨æ·±å…¥ä»£ç ä¹‹å‰ï¼Œå¿…é¡»å…ˆç†è§£ **Block-Based Autoregressive Generationï¼ˆåŸºäºå—çš„è‡ªå›å½’ç”Ÿæˆï¼‰** çš„æ€æƒ³ï¼š
1.  **åˆ†å—**ï¼šæˆ‘ä»¬ä¸ä¼šä¸€æ¬¡æ€§ç”Ÿæˆæ‰€æœ‰è§†é¢‘å¸§ï¼ˆå› ä¸ºæ˜¾å­˜ä¸å¤Ÿï¼Œä¸”éš¾ä»¥ç»´æŒé•¿æ—¶åºä¸€è‡´æ€§ï¼‰ã€‚æˆ‘ä»¬å°†è§†é¢‘åˆ‡åˆ†ä¸ºä¸€ä¸ªä¸ª **Block**ï¼Œæ¯ä¸ª Block é€šå¸¸åŒ…å« 3 å¸§ã€‚
2.  **æ¥é¾™**ï¼šæˆ‘ä»¬å…ˆç”Ÿæˆç¬¬ 1 ä¸ª Blockï¼ˆ0-3å¸§ï¼‰ã€‚ç”Ÿæˆå¥½åï¼ŒæŠŠå®ƒçš„ç‰¹å¾å›ºå®šä¸‹æ¥ï¼ˆå­˜å…¥ KV Cacheï¼‰ã€‚
3.  **ä¾èµ–**ï¼šç”Ÿæˆç¬¬ 2 ä¸ª Blockï¼ˆ3-6å¸§ï¼‰æ—¶ï¼Œæ¨¡å‹ä¼šâ€œå›å¤´çœ‹â€ç¬¬ 1 ä¸ª Block çš„ç‰¹å¾ï¼Œä»è€Œä¿è¯è¿è´¯æ€§ã€‚
4.  **å¾ªç¯**ï¼šå¦‚æ­¤å¾€å¤ï¼Œç›´åˆ°ç”Ÿæˆæ‰€æœ‰å¸§ã€‚

### 2.1 å‡½æ•°ç­¾åä¸è¾“å…¥å‚æ•°

```python
    def inference(
        self,
        noise: torch.Tensor,
        text_prompts: List[str],
        initial_latent: Optional[torch.Tensor] = None,
        return_latents: bool = False,
        profile: bool = False,
        low_memory: bool = False,
    ) -> torch.Tensor:
```

*   **`noise`**: `[Batch, Total_Frames, Channels, Height, Width]`
    *   è¿™æ˜¯æ‰©æ•£æ¨¡å‹çš„èµ·å§‹ç‚¹ï¼ˆçº¯é«˜æ–¯å™ªå£°ï¼‰ã€‚
    *   **é‡è¦**ï¼šå®ƒçš„å½¢çŠ¶ç›´æ¥å†³å®šäº†æˆ‘ä»¬è¦ç”Ÿæˆå¤šé•¿çš„è§†é¢‘ï¼ˆç”± `Total_Frames` å†³å®šï¼‰ã€‚
*   **`text_prompts`**: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬æç¤ºã€‚
*   **`initial_latent`**: `[Batch, Input_Frames, C, H, W]`
    *   **I2V (å›¾ç”Ÿè§†é¢‘) æ¨¡å¼**ï¼šè¿™é‡Œä¼ å…¥é¦–å¸§å›¾åƒçš„ Latentã€‚
    *   **Video Extension (è§†é¢‘æ‰©å……) æ¨¡å¼**ï¼šè¿™é‡Œä¼ å…¥å‰ä¸€æ®µè§†é¢‘çš„ Latentã€‚
    *   **T2V (æ–‡ç”Ÿè§†é¢‘) æ¨¡å¼**ï¼šè¿™é‡Œæ˜¯ `None`ã€‚
*   **`return_latents`**: è°ƒè¯•ç”¨ï¼Œå¦‚æœä¸º `True`ï¼Œé™¤äº†è¿”å›æœ€ç»ˆåƒç´ è§†é¢‘ï¼Œè¿˜è¿”å› Latent å¼ é‡ã€‚

### 2.2 å˜é‡åˆå§‹åŒ–ä¸åˆ†å—è®¡ç®— (L72-83)

æˆ‘ä»¬éœ€è¦è®¡ç®—â€œæ€»å…±è¦å¾ªç¯å¤šå°‘æ¬¡â€ï¼Œå³æœ‰å¤šå°‘ä¸ª Blockã€‚

```python
        # è·å–æ€»å¸§æ•°
        batch_size, num_frames, num_channels, height, width = noise.shape
        
        # è®¡ç®— Block æ•°é‡
        if not self.independent_first_frame or (self.independent_first_frame and initial_latent is not None):
            #è¿™æ˜¯æœ€å¸¸è§çš„æƒ…å†µ
            # å‡è®¾ num_frames=21, per_block=3, åˆ™ num_blocks = 7
            assert num_frames % self.num_frame_per_block == 0
            num_blocks = num_frames // self.num_frame_per_block
        else:
            # è¿™æ˜¯ä¸€ä¸ªæå°‘ç”¨çš„æµ‹è¯•åˆ†æ”¯ï¼Œç¬¬ä¸€å¸§ç‹¬ç«‹ç”Ÿæˆï¼Œä¸ç”¨ç®¡å®ƒ
            assert (num_frames - 1) % self.num_frame_per_block == 0
            num_blocks = (num_frames - 1) // self.num_frame_per_block
```

### 2.3 æ–‡æœ¬ç¼–ç  (L84-86)

```python
        conditional_dict = self.text_encoder(
            text_prompts=text_prompts
        )
```
*   è°ƒç”¨ T5 Encoderï¼ŒæŠŠæ–‡æœ¬å˜æˆ Embeddingsã€‚
*   è¿™äº› Embeddings ä¼šä¸€ç›´å¤ç”¨ï¼ŒæŒ‡å¯¼æ¯ä¸€ä¸ª Block çš„ç”Ÿæˆã€‚

### 2.4 è¾“å‡ºå®¹å™¨å‡†å¤‡ (L92-96)

```python
        output = torch.zeros(
            [batch_size, num_output_frames, num_channels, height, width],
            device=noise.device,
            dtype=noise.dtype
        )
```
*   åˆ›å»ºä¸€ä¸ªå…¨é›¶çš„å¼ é‡ä½œä¸ºç”»å¸ƒã€‚
*   æœ€ç»ˆç”Ÿæˆçš„æ¯ä¸€å¸§éƒ½ä¼šè¢«â€œå¡«â€è¿›è¿™ä¸ª `output` é‡Œã€‚

---

### ğŸ”¥ 2.5 æ­¥éª¤ 1: KV Cache çš„åˆå§‹åŒ– (L111-133)

è¿™æ˜¯å› æœæ¨ç†ï¼ˆCausal Inferenceï¼‰çš„åŸºç¡€è®¾æ–½å»ºè®¾ã€‚

```python
        if self.kv_cache1 is None:
            # === Case A: ç¬¬ä¸€æ¬¡è¿è¡Œ ===
            # åˆ†é…æ˜¾å­˜ã€‚kv_cache1 æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ç­‰äº Transformer å±‚æ•° (30)ã€‚
            # æ¯ä¸€å±‚åŒ…å«ä¸€ä¸ªå­—å…¸ï¼š{'k': ..., 'v': ..., 'global_end_index': ...}
            self._initialize_kv_cache(...)
            self._initialize_crossattn_cache(...)
        else:
            # === Case B: æ˜¾å­˜å¤ç”¨ ===
            # å¦‚æœä¹‹å‰çš„æ¨ç†å·²ç»åˆ†é…è¿‡ cacheï¼Œæˆ‘ä»¬ç›´æ¥å¤ç”¨ç‰©ç†æ˜¾å­˜ï¼Œ
            # åªæ˜¯æŠŠâ€œæŒ‡é’ˆâ€(global_end_index) å½’é›¶ã€‚
            # è¿™æ˜¯ä¸ºäº†æè‡´çš„æ€§èƒ½ä¼˜åŒ–ï¼Œé¿å…åå¤ malloc/free å¯¼è‡´æ˜¾å­˜ç¢ç‰‡ã€‚
            for block_index in range(self.num_transformer_blocks):
                self.crossattn_cache[block_index]["is_init"] = False
            for block_index in range(len(self.kv_cache1)):
                 # æŠŠâ€œå†™å…¥ä½ç½®â€æŒ‡é’ˆå½’é›¶ï¼Œç›¸å½“äºæ¸…ç©ºäº†å†…å®¹ï¼Œä½†æ²¡é‡Šæ”¾å†…å­˜
                self.kv_cache1[block_index]["global_end_index"] = torch.tensor([0], ...)
```

---

### ğŸ”¥ 2.6 æ­¥éª¤ 2: é¢„å¡«å…… (Prefill Context) (L134-170)

**åœºæ™¯**ï¼šå‡è®¾æˆ‘ä»¬è¦åŸºäºä¸€å¼ å›¾ç”Ÿæˆè§†é¢‘ (I2V)ï¼Œæˆ–è€…åŸºäºå‰ 3 ç§’ç”Ÿæˆå 3 ç§’ã€‚
**é—®é¢˜**ï¼šKV Cache ç°åœ¨æ˜¯ç©ºçš„ï¼ˆæˆ–å·²å½’é›¶ï¼‰ã€‚æ¨¡å‹å¦‚æœç›´æ¥å¼€å§‹ç”Ÿæˆåç»­å¸§ï¼Œå®ƒä¸çŸ¥é“å‰é¢çš„å†å²ä¿¡æ¯ã€‚
**è§£å†³**ï¼šæˆ‘ä»¬éœ€è¦æŠŠå·²çŸ¥çš„å†å²å¸§ï¼ˆ`initial_latent`ï¼‰å…ˆâ€œè¿‡ä¸€éâ€æ¨¡å‹ï¼ŒæŠŠå®ƒä»¬çš„ç‰¹å¾å­˜è¿› KV Cacheã€‚

```python
        current_start_frame = 0
        if initial_latent is not None:
             # è®¾ç½® timestep ä¸º 0ã€‚
             # åœ¨ Diffusion ä¸­ï¼Œt=0 æ„å‘³ç€â€œæ²¡æœ‰å™ªå£°â€ï¼Œå³æ¸…æ™°å›¾åƒã€‚
             # æˆ‘ä»¬å‘Šè¯‰æ¨¡å‹ï¼šâ€œå˜¿ï¼Œè¿™æ˜¯å®Œç¾çš„å†å²æ•°æ®ï¼Œè¯·è®°ä½å®ƒã€‚â€
            timestep = torch.ones([batch_size, 1], ...) * 0

            # éå†æ‰€æœ‰è¾“å…¥çš„å†å²å—
            for _ in range(num_input_blocks):
                # åˆ‡ç‰‡å–å‡ºå½“å‰è¦å¤„ç†çš„é‚£å‡ å¸§å†å²æ•°æ®
                current_ref_latents = initial_latent[:, current_start:current_end]
                
                # å¡«å…¥ output ç”»å¸ƒ
                output[:, current_start:current_end] = current_ref_latents
                
                # === å…³é”®åŠ¨ä½œ ===
                # è¿è¡Œ Generatorã€‚æ³¨æ„è¿™é‡Œæ²¡æœ‰æ¥æ”¶è¿”å›å€¼ï¼
                # æˆ‘ä»¬ä¸åœ¨ä¹å®ƒçš„è¾“å‡ºï¼Œåªåœ¨ä¹å‰¯ä½œç”¨ï¼šæ›´æ–° self.kv_cache1
                self.generator(
                    noisy_image_or_video=current_ref_latents,
                    conditional_dict=conditional_dict,
                    timestep=timestep * 0, # t=0, å¼ºåˆ¶ Teacher Forcing
                    kv_cache=self.kv_cache1, # ä¼ å…¥ Cache å¯¹è±¡ï¼Œå†…éƒ¨ä¼šè‡ªåŠ¨å†™å…¥
                    crossattn_cache=self.crossattn_cache,
                    # ...
                )
                # ç§»åŠ¨æŒ‡é’ˆ
                current_start_frame += self.num_frame_per_block
```

---

### ğŸ”¥ğŸ”¥ 2.7 æ­¥éª¤ 3: æ ¸å¿ƒæ—¶åºå»å™ªå¾ªç¯ (L176-245)

è¿™æ˜¯æ•´ä¸ªæ¨ç†è¿‡ç¨‹çš„å¿ƒè„ï¼Œå®ç°äº† **Self-Forcing** æœºåˆ¶ã€‚

å®ƒæœ‰ä¸¤å±‚å¾ªç¯ï¼š
1.  **Block å¾ªç¯**ï¼šæŒ‰æ—¶é—´é¡ºåºï¼Œä¸€æ®µä¸€æ®µç”Ÿæˆè§†é¢‘ã€‚
2.  **Denoising å¾ªç¯**ï¼šåœ¨æ¯ä¸€æ®µå†…éƒ¨ï¼Œä»å™ªå£°é€æ­¥è¿˜åŸä¸ºå›¾åƒã€‚

```python
        # å‡†å¤‡ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯”å¦‚ [3, 3, 3, 3, 3, 3, 3]
        all_num_frames = [self.num_frame_per_block] * num_blocks

        # === å¤–å±‚å¾ªç¯ï¼šéå†æ¯ä¸€ä¸ª Block ===
        for current_num_frames in all_num_frames:
            
            # 1. å‡†å¤‡å™ªå£°
            # å–å‡ºå½“å‰è¿™ 3 å¸§å¯¹åº”çš„çº¯å™ªå£°
            noisy_input = noise[:, current_start_frame : current_start_frame + 3]

            # === å†…å±‚å¾ªç¯ï¼šéå†å»å™ªæ­¥æ•° (ä¾‹å¦‚ 4 æ­¥) ===
            # denoising_step_list å¯èƒ½æ˜¯ [1000, 750, 500, 250]
            for index, current_timestep in enumerate(self.denoising_step_list):
                
                # 2. æ„é€  timestep å¼ é‡
                timestep = torch.ones(..., dtype=torch.int64) * current_timestep

                # 3. é¢„æµ‹å™ªå£°/åŸå›¾ (Model Prediction)
                _, denoised_pred = self.generator(
                    noisy_image_or_video=noisy_input, # å½“å‰å……æ»¡å™ªå£°çš„ 3 å¸§
                    timestep=timestep,
                    kv_cache=self.kv_cache1, # ã€è¿™æ˜¯å…³é”®ã€‘
                    # è¿™é‡Œä¼ å…¥ kv_cache1ï¼Œæ¨¡å‹ä¼šè¯»å–ä¹‹å‰çš„å†å²ä¿¡æ¯ï¼
                    # ä½†æ˜¯ï¼å› ä¸ºæ­£åœ¨å»å™ªä¸­ï¼Œç»“æœè¿˜ä¸ç¡®å®šï¼Œæ‰€ä»¥æ¨¡å‹ã€ä¸ä¼šã€‘
                    # æŠŠå½“å‰è¿™ 3 å¸§å†™å…¥ Cacheï¼Œåªä¼šè¯»å–å‰é¢çš„ã€‚
                    # ...
                )

                # 4. è°ƒåº¦å™¨æ›´æ–° (Step)
                if index < len(self.denoising_step_list) - 1:
                    # å¦‚æœè¿˜æ²¡åˆ°æœ€åä¸€æ­¥ï¼Œå°±åŠ ç‚¹å™ªï¼Œå‡†å¤‡ä¸‹ä¸€æ¬¡è¿­ä»£
                    # ç±»ä¼¼äº x_{t-1} = x_0 + noise
                    next_timestep = self.denoising_step_list[index + 1]
                    noisy_input = self.scheduler.add_noise(
                        denoised_pred, ..., next_timestep
                    )
                else:
                    # æœ€åä¸€æ­¥ï¼Œdenoised_pred å°±æ˜¯æˆ‘ä»¬ç»ˆäºç”Ÿæˆå¥½çš„ clean latents
                    pass

            # -------------------------------------------------------
            #  åˆ°è¿™é‡Œï¼Œå½“å‰ Block (3å¸§) å·²ç»å®Œå…¨ç”Ÿæˆå®Œæ¯•äº†ï¼
            #  ä½†æ˜¯ï¼ŒKV Cache é‡Œè¿˜æ²¡æœ‰è¿™ 3 å¸§çš„ä¿¡æ¯ã€‚
            #  ä¸ºäº†è®©ä¸‹ä¸€ä¸ª Block èƒ½å‚è€ƒè¿™ 3 å¸§ï¼Œæˆ‘ä»¬å¿…é¡»æŠŠå®ƒä»¬å­˜è¿›å»ã€‚
            # -------------------------------------------------------

            # 5. è®°å½•ç»“æœ
            output[:, current_start : current_end] = denoised_pred

            # 6. Self-Forcing æ›´æ–° (Step 3.3)
            # å†æ¬¡æ„é€ ä¸€ä¸ª t=0 (æˆ–æå°å€¼) çš„ timestep
            context_timestep = torch.ones_like(timestep) * self.args.context_noise 

            # å†è·‘ä¸€æ¬¡æ¨¡å‹ï¼è¿™æ˜¯ä¸€æ¬¡é¢å¤–çš„ overheadã€‚
            # è¿™æ¬¡è¾“å…¥çš„æ˜¯åˆšåˆšç”Ÿæˆçš„ perfect result (denoised_pred)ã€‚
            self.generator(
                noisy_image_or_video=denoised_pred,
                conditional_dict=conditional_dict,
                timestep=context_timestep,
                kv_cache=self.kv_cache1, 
                # ã€æ³¨æ„ã€‘è¿™ä¸€æ¬¡è°ƒç”¨ï¼ŒGenerator å†…éƒ¨é€»è¾‘ä¼šæ£€æµ‹åˆ°è¾“å…¥æ˜¯ clean çš„
                # (æˆ–è€…æ ¹æ®å†…éƒ¨æ ‡å¿—ä½)ï¼Œå®ƒä¼šå°†è¿™ 3 å¸§çš„ Key/Value è®¡ç®—å‡ºæ¥ï¼Œ
                # å¹¶è¿½åŠ å†™å…¥åˆ° kv_cache1 çš„æœ«å°¾ï¼ï¼ï¼
                # ...
            )

            # 7. ç§»åŠ¨æŒ‡é’ˆï¼Œå¤„ç†ä¸‹ä¸€ä¸ª Block
            current_start_frame += current_num_frames
```

### 2.8 æ­¥éª¤ 4: è§£ç  (Video Decoding) (L254-256)

æ­¤æ—¶ `output` åŒ…å«äº†æ‰€æœ‰ç”Ÿæˆçš„ Latent Framesã€‚æˆ‘ä»¬éœ€è¦ç”¨ VAE æŠŠå®ƒä»¬å˜å›äººçœ¼å¯çœ‹çš„åƒç´ ã€‚

```python
        # VAE è§£ç : Latent -> Pixel
        # output shape: [Batch, Frames, Channels, Height_Latent, Width_Latent]
        video = self.vae.decode_to_pixel(output, use_cache=False)
        
        # å½’ä¸€åŒ–: [-1, 1] -> [0, 1]
        video = (video * 0.5 + 0.5).clamp(0, 1)
```

---

## 3. è¾…åŠ©å‡½æ•°è§£æ (L278-313)

### `_initialize_kv_cache`
*   **åˆ†é…**ï¼šé¢„å…ˆåˆ†é…å¥½èƒ½å®¹çº³æ•´ä¸ªè§†é¢‘æ‰€æœ‰å¸§çš„å·¨å¤§ Tensorã€‚
    *   ä¾‹å¦‚ï¼š`Size = 32760` (å¯¹åº” approx 21 å¸§ * 1560 tokens)ã€‚
*   **å¥½å¤„**ï¼šç›¸æ¯”äº python list `append` æˆ–è€… torch `cat`ï¼Œè¿™ç§**é™æ€é¢„åˆ†é…**å¤§å¤§å‡å°‘äº†æ˜¾å­˜ç¢ç‰‡ï¼Œè¿™å¯¹æ˜¾å­˜æå…¶ç´§å¼ çš„è§†é¢‘ç”Ÿæˆä»»åŠ¡è‡³å…³é‡è¦ã€‚

### `_initialize_crossattn_cache`
*   **ç”¨é€”**ï¼šç¼“å­˜æ–‡æœ¬ Prompt çš„ Attention ç»“æœã€‚
*   **åŸå› **ï¼šæ–‡æœ¬ Prompt ä»å¤´åˆ°å°¾æ˜¯ä¸å˜çš„ã€‚å¦‚æœæ²¡æœ‰è¿™ä¸ª Cacheï¼Œæ¯ä¸€å¸§éƒ½è¦é‡æ–°è®¡ç®—ä¸€é Text-to-Image Attentionï¼Œæµªè´¹ç®—åŠ›ã€‚æœ‰äº†å®ƒï¼Œåªéœ€è¦è®¡ç®—ä¸€æ¬¡ã€‚

---

## æ€»ç»“

è¿™å°±æ˜¯ Self-Forcing çš„ç²¾é«“ï¼š
1.  **Split**: æŠŠé•¿éš¾ä»»åŠ¡æ‹†æˆçŸ­ä»»åŠ¡ (Block)ã€‚
2.  **Generate**: æ¯ä¸€ä¸ªçŸ­ä»»åŠ¡ç‹¬ç«‹å»å™ªç”Ÿæˆã€‚
3.  **Force**: ç”Ÿæˆå®Œåï¼Œå¼ºåˆ¶æŠŠç»“æœå½“ä½œâ€œçœŸå€¼â€å†™å…¥è®°å¿† (KV Cache)ã€‚
4.  **Next**: ä¸‹ä¸€ä¸ªçŸ­ä»»åŠ¡è¯»å–è®°å¿†ï¼ŒåŸºäºå†å²ç»§ç»­ç”Ÿæˆã€‚

è¿™ç§æœºåˆ¶å®Œç¾è§£å†³äº†é•¿è§†é¢‘ç”Ÿæˆä¸­çš„æ˜¾å­˜çˆ†ç‚¸é—®é¢˜ï¼ˆä¸éœ€è¦ä¸€æ¬¡æ€§æŠŠæ‰€æœ‰å¸§æ”¾å…¥æ˜¾å­˜è¿›è¡Œ Attentionï¼‰ï¼ŒåŒæ—¶ä¹Ÿä¿è¯äº†è§†é¢‘åœ¨æ—¶é—´ä¸Šçš„è¿è´¯æ€§ã€‚
