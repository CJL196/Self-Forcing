# inference.py é€æ®µè§£æ

æœ¬æ–‡æ¡£é€æ®µè§£é‡Š `inference.py` æ–‡ä»¶çš„æ¯ä¸ªéƒ¨åˆ†ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¼å…¥ä¾èµ– (L1-20)

```python
import argparse
import torch
import os
from omegaconf import OmegaConf
from tqdm import tqdm
from torchvision import transforms
from torchvision.io import write_video
from einops import rearrange
import torch.distributed as dist
from torch.utils.data import DataLoader, SequentialSampler
from torch.utils.data.distributed import DistributedSampler

from pipeline import (
    CausalDiffusionInferencePipeline,
    CausalInferencePipeline,
)
from utils.dataset import TextDataset, TextImagePairDataset
from utils.misc import set_seed

from demo_utils.memory import gpu, get_cuda_free_memory_gb, DynamicSwapInstaller
```

### è§£é‡Š

| æ¨¡å— | ä½œç”¨ |
|------|------|
| `argparse` | è§£æå‘½ä»¤è¡Œå‚æ•° |
| `torch` | PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| `OmegaConf` | åŠ è½½ YAML é…ç½®æ–‡ä»¶ |
| `tqdm` | æ˜¾ç¤ºè¿›åº¦æ¡ |
| `transforms` | å›¾åƒé¢„å¤„ç†ï¼ˆç”¨äº I2Vï¼‰ |
| `write_video` | å°† tensor ä¿å­˜ä¸º MP4 è§†é¢‘ |
| `einops.rearrange` | å¼ é‡ç»´åº¦é‡æ’ |
| `torch.distributed` | å¤š GPU åˆ†å¸ƒå¼æ¨ç†æ”¯æŒ |
| `CausalInferencePipeline` | **æ ¸å¿ƒ**ï¼šå°‘æ­¥å› æœæ¨ç†ç®¡çº¿ |
| `CausalDiffusionInferencePipeline` | å¤šæ­¥æ‰©æ•£æ¨ç†ç®¡çº¿ |
| `TextDataset` | åŠ è½½æ–‡æœ¬æç¤ºæ•°æ®é›† |
| `TextImagePairDataset` | åŠ è½½å›¾æ–‡å¯¹æ•°æ®é›†ï¼ˆI2Vï¼‰ |
| `DynamicSwapInstaller` | ä½æ˜¾å­˜æ—¶åŠ¨æ€æ¢å…¥æ¢å‡ºæ¨¡å‹ |

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šå‘½ä»¤è¡Œå‚æ•°å®šä¹‰ (L22-36)

```python
parser = argparse.ArgumentParser()
parser.add_argument("--config_path", type=str, help="Path to the config file")
parser.add_argument("--checkpoint_path", type=str, help="Path to the checkpoint folder")
parser.add_argument("--data_path", type=str, help="Path to the dataset")
parser.add_argument("--extended_prompt_path", type=str, help="Path to the extended prompt")
parser.add_argument("--output_folder", type=str, help="Output folder")
parser.add_argument("--num_output_frames", type=int, default=21,
                    help="Number of overlap frames between sliding windows")
parser.add_argument("--i2v", action="store_true", help="Whether to perform I2V (or T2V by default)")
parser.add_argument("--use_ema", action="store_true", help="Whether to use EMA parameters")
parser.add_argument("--seed", type=int, default=0, help="Random seed")
parser.add_argument("--num_samples", type=int, default=1, help="Number of samples to generate per prompt")
parser.add_argument("--save_with_index", action="store_true",
                    help="Whether to save the video using the index or prompt as the filename")
args = parser.parse_args()
```

### å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--config_path` | str | å¿…å¡« | é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ `configs/self_forcing_dmd.yaml`ï¼‰ |
| `--checkpoint_path` | str | å¯é€‰ | æ¨¡å‹æƒé‡è·¯å¾„ |
| `--data_path` | str | å¿…å¡« | è¾“å…¥æç¤ºæ–‡ä»¶è·¯å¾„ |
| `--extended_prompt_path` | str | å¯é€‰ | æ‰©å±•æç¤ºæ–‡ä»¶è·¯å¾„ |
| `--output_folder` | str | å¿…å¡« | è¾“å‡ºè§†é¢‘ç›®å½• |
| `--num_output_frames` | int | 21 | ç”Ÿæˆçš„æ½œåœ¨ç©ºé—´å¸§æ•° |
| `--i2v` | flag | False | æ˜¯å¦ä½¿ç”¨å›¾ç”Ÿè§†é¢‘æ¨¡å¼ |
| `--use_ema` | flag | False | æ˜¯å¦ä½¿ç”¨ EMA æƒé‡ |
| `--seed` | int | 0 | éšæœºç§å­ |
| `--num_samples` | int | 1 | æ¯ä¸ªæç¤ºç”Ÿæˆå‡ ä¸ªè§†é¢‘ |
| `--save_with_index` | flag | False | æ–‡ä»¶åç”¨ç´¢å¼•è¿˜æ˜¯æç¤ºæ–‡æœ¬ |

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šåˆ†å¸ƒå¼åˆå§‹åŒ– (L38-50)

```python
# Initialize distributed inference
if "LOCAL_RANK" in os.environ:
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ["LOCAL_RANK"])
    torch.cuda.set_device(local_rank)
    device = torch.device(f"cuda:{local_rank}")
    world_size = dist.get_world_size()
    set_seed(args.seed + local_rank)
else:
    device = torch.device("cuda")
    local_rank = 0
    world_size = 1
    set_seed(args.seed)
```

### è§£é‡Š
- **åˆ†å¸ƒå¼æ¨¡å¼**ï¼šé€šè¿‡ `torchrun` å¯åŠ¨æ—¶ä¼šè®¾ç½® `LOCAL_RANK` ç¯å¢ƒå˜é‡
- **å•å¡æ¨¡å¼**ï¼šç›´æ¥ä½¿ç”¨ `cuda` è®¾å¤‡
- **éšæœºç§å­**ï¼šåˆ†å¸ƒå¼æ—¶æ¯ä¸ª GPU ä½¿ç”¨ä¸åŒç§å­ï¼ˆ`seed + local_rank`ï¼‰ï¼Œç¡®ä¿ç”Ÿæˆä¸åŒè§†é¢‘

---

## ç¬¬å››éƒ¨åˆ†ï¼šæ˜¾å­˜æ£€æµ‹ä¸æ¢¯åº¦ç¦ç”¨ (L52-55)

```python
print(f'Free VRAM {get_cuda_free_memory_gb(gpu)} GB')
low_memory = get_cuda_free_memory_gb(gpu) < 40

torch.set_grad_enabled(False)
```

### è§£é‡Š
- **æ˜¾å­˜æ£€æµ‹**ï¼šè‹¥å¯ç”¨æ˜¾å­˜ < 40GBï¼Œå¯ç”¨ä½æ˜¾å­˜æ¨¡å¼
- **ç¦ç”¨æ¢¯åº¦**ï¼šæ¨ç†æ—¶ä¸éœ€è¦è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœæ˜¾å­˜

---

## ç¬¬äº”éƒ¨åˆ†ï¼šåŠ è½½é…ç½®æ–‡ä»¶ (L57-59)

```python
config = OmegaConf.load(args.config_path)
default_config = OmegaConf.load("configs/default_config.yaml")
config = OmegaConf.merge(default_config, config)
```

### è§£é‡Š
1. åŠ è½½ç”¨æˆ·æŒ‡å®šçš„é…ç½®æ–‡ä»¶ï¼ˆå¦‚ `self_forcing_dmd.yaml`ï¼‰
2. åŠ è½½é»˜è®¤é…ç½®
3. åˆå¹¶é…ç½®ï¼ˆç”¨æˆ·é…ç½®è¦†ç›–é»˜è®¤é…ç½®ï¼‰

---

## ç¬¬å…­éƒ¨åˆ†ï¼šåˆå§‹åŒ–æ¨ç†ç®¡çº¿ (L61-67)

```python
# Initialize pipeline
if hasattr(config, 'denoising_step_list'):
    # Few-step inference
    pipeline = CausalInferencePipeline(config, device=device)
else:
    # Multi-step diffusion inference
    pipeline = CausalDiffusionInferencePipeline(config, device=device)
```

### è§£é‡Š
- **æœ‰ `denoising_step_list`**ï¼šä½¿ç”¨å°‘æ­¥æ¨ç†ï¼ˆSelf-Forcing è’¸é¦æ¨¡å‹ï¼Œå¦‚ 4 æ­¥ï¼‰
- **æ—  `denoising_step_list`**ï¼šä½¿ç”¨æ ‡å‡†å¤šæ­¥æ‰©æ•£æ¨ç†ï¼ˆå¦‚ 50 æ­¥ï¼‰

ç®¡çº¿å†…éƒ¨ä¼šåˆå§‹åŒ–ä¸‰ä¸ªæ¨¡å‹ï¼š
1. `generator`ï¼šæ‰©æ•£ç”Ÿæˆå™¨ï¼ˆCausalWanModelï¼‰
2. `text_encoder`ï¼šæ–‡æœ¬ç¼–ç å™¨ï¼ˆUMT5-XXLï¼‰
3. `vae`ï¼šå˜åˆ†è‡ªç¼–ç å™¨

---

## ç¬¬ä¸ƒéƒ¨åˆ†ï¼šåŠ è½½æ¨¡å‹æƒé‡ (L69-79)

```python
if args.checkpoint_path:
    state_dict = torch.load(args.checkpoint_path, map_location="cpu")
    pipeline.generator.load_state_dict(state_dict['generator' if not args.use_ema else 'generator_ema'])

pipeline = pipeline.to(dtype=torch.bfloat16)
if low_memory:
    DynamicSwapInstaller.install_model(pipeline.text_encoder, device=gpu)
else:
    pipeline.text_encoder.to(device=gpu)
pipeline.generator.to(device=gpu)
pipeline.vae.to(device=gpu)
```

### è§£é‡Š
1. **åŠ è½½æƒé‡**ï¼šä» checkpoint åŠ è½½ generator æƒé‡ï¼ˆå¯é€‰ EMA ç‰ˆæœ¬ï¼‰
2. **ç²¾åº¦è½¬æ¢**ï¼šè½¬ä¸º bfloat16 å‡å°‘æ˜¾å­˜å ç”¨
3. **æ˜¾å­˜ç®¡ç†**ï¼š
   - ä½æ˜¾å­˜æ¨¡å¼ï¼štext_encoder åŠ¨æ€æ¢å…¥æ¢å‡º
   - æ­£å¸¸æ¨¡å¼ï¼šæ‰€æœ‰æ¨¡å‹å¸¸é©» GPU

---

## ç¬¬å…«éƒ¨åˆ†ï¼šåˆ›å»ºæ•°æ®é›† (L82-100)

```python
# Create dataset
if args.i2v:
    assert not dist.is_initialized(), "I2V does not support distributed inference yet"
    transform = transforms.Compose([
        transforms.Resize((480, 832)),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5])
    ])
    dataset = TextImagePairDataset(args.data_path, transform=transform)
else:
    dataset = TextDataset(prompt_path=args.data_path, extended_prompt_path=args.extended_prompt_path)
num_prompts = len(dataset)
print(f"Number of prompts: {num_prompts}")

if dist.is_initialized():
    sampler = DistributedSampler(dataset, shuffle=False, drop_last=True)
else:
    sampler = SequentialSampler(dataset)
dataloader = DataLoader(dataset, batch_size=1, sampler=sampler, num_workers=0, drop_last=False)
```

### è§£é‡Š
- **I2V æ¨¡å¼**ï¼šåŠ è½½å›¾æ–‡å¯¹ï¼Œå›¾åƒ resize åˆ° 480Ã—832
- **T2V æ¨¡å¼**ï¼šåªåŠ è½½æ–‡æœ¬æç¤º
- **åˆ†å¸ƒå¼é‡‡æ ·**ï¼šå¤š GPU æ—¶è‡ªåŠ¨åˆ†é…æ•°æ®

---

## ç¬¬ä¹éƒ¨åˆ†ï¼šåˆ›å»ºè¾“å‡ºç›®å½• (L102-107)

```python
# Create output directory (only on main process to avoid race conditions)
if local_rank == 0:
    os.makedirs(args.output_folder, exist_ok=True)

if dist.is_initialized():
    dist.barrier()
```

### è§£é‡Š
- åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•ï¼Œé¿å…ç«äº‰æ¡ä»¶
- `barrier()` åŒæ­¥æ‰€æœ‰è¿›ç¨‹ï¼Œç¡®ä¿ç›®å½•åˆ›å»ºå®Œæˆåå†ç»§ç»­

---

## ç¬¬åéƒ¨åˆ†ï¼šè¾…åŠ©å‡½æ•° (L110-120)

```python
def encode(self, videos: torch.Tensor) -> torch.Tensor:
    device, dtype = videos[0].device, videos[0].dtype
    scale = [self.mean.to(device=device, dtype=dtype),
             1.0 / self.std.to(device=device, dtype=dtype)]
    output = [
        self.model.encode(u.unsqueeze(0), scale).float().squeeze(0)
        for u in videos
    ]
    output = torch.stack(output, dim=0)
    return output
```

### è§£é‡Š
è¿™æ˜¯ä¸€ä¸ª VAE ç¼–ç è¾…åŠ©å‡½æ•°ï¼ˆå½“å‰ä»£ç ä¸­æœªè¢«ä½¿ç”¨ï¼Œå¯èƒ½æ˜¯é—ç•™ä»£ç ï¼‰ã€‚

---

## ç¬¬åä¸€éƒ¨åˆ†ï¼šä¸»æ¨ç†å¾ªç¯ (L123-193) â€” é€è¡Œè¯¦è§£

### L123: ä¸»å¾ªç¯å¼€å§‹

```python
for i, batch_data in tqdm(enumerate(dataloader), disable=(local_rank != 0)):
```

| å…ƒç´  | è¯´æ˜ |
|------|------|
| `i` | å¾ªç¯è®¡æ•°å™¨ï¼ˆä» 0 å¼€å§‹ï¼‰ |
| `batch_data` | ä» DataLoader è·å–çš„ä¸€æ‰¹æ•°æ®ï¼ˆdict æ ¼å¼ï¼‰ |
| `tqdm` | è¿›åº¦æ¡æ˜¾ç¤º |
| `disable=(local_rank != 0)` | åªåœ¨ä¸»è¿›ç¨‹ï¼ˆrank=0ï¼‰æ˜¾ç¤ºè¿›åº¦æ¡ï¼Œé¿å…å¤š GPU æ—¶é‡å¤è¾“å‡º |

---

### L124: è·å–æ ·æœ¬ç´¢å¼•

```python
idx = batch_data['idx'].item()
```

- `batch_data['idx']` æ˜¯ä¸€ä¸ªåªæœ‰ 1 ä¸ªå…ƒç´ çš„ tensor
- `.item()` å°†å…¶è½¬æ¢ä¸º Python int
- `idx` ç”¨äºåç»­åˆ¤æ–­æ˜¯å¦æ˜¯æœ‰æ•ˆæ ·æœ¬ï¼ˆè€Œéå¡«å……çš„ dummy æ•°æ®ï¼‰

---

### L126-131: è§£åŒ…æ‰¹æ¬¡æ•°æ®

```python
# For DataLoader batch_size=1, the batch_data is already a single item, but in a batch container
# Unpack the batch data for convenience
if isinstance(batch_data, dict):
    batch = batch_data
elif isinstance(batch_data, list):
    batch = batch_data[0]  # First (and only) item in the batch
```

ç”±äº `batch_size=1`ï¼Œæ•°æ®å·²ç»æ˜¯å•ä¸ªæ ·æœ¬ï¼Œè¿™é‡Œåªæ˜¯å…¼å®¹ä¸åŒçš„æ•°æ®æ ¼å¼ã€‚

---

### L133-134: åˆå§‹åŒ–å­˜å‚¨å˜é‡

```python
all_video = []
num_generated_frames = 0  # Number of generated (latent) frames
```

- `all_video` å­˜å‚¨ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µï¼ˆç”¨äºé•¿è§†é¢‘æ‹¼æ¥ï¼Œå½“å‰ä»£ç æœªä½¿ç”¨å¤šæ®µï¼‰
- `num_generated_frames` ç»Ÿè®¡ç”Ÿæˆçš„æ½œåœ¨å¸§æ•°

---

### L136-150: å›¾ç”Ÿè§†é¢‘ (I2V) åˆ†æ”¯

```python
if args.i2v:
    # For image-to-video, batch contains image and caption
    prompt = batch['prompts'][0]  # Get caption from batch
    prompts = [prompt] * args.num_samples
```

- æå–æ–‡æœ¬æè¿°
- å¤åˆ¶ `num_samples` ä»½ï¼ˆç”¨äºç”Ÿæˆå¤šä¸ªæ ·æœ¬ï¼‰

```python
    # Process the image
    image = batch['image'].squeeze(0).unsqueeze(0).unsqueeze(2).to(device=device, dtype=torch.bfloat16)
```

**ç»´åº¦å˜æ¢è¯¦è§£**ï¼š
```
batch['image']          : [1, 3, 480, 832]        # DataLoader è¾“å‡º
  .squeeze(0)           : [3, 480, 832]           # ç§»é™¤ batch ç»´åº¦
  .unsqueeze(0)         : [1, 3, 480, 832]        # é‡æ–°æ·»åŠ  batch ç»´åº¦
  .unsqueeze(2)         : [1, 3, 1, 480, 832]     # æ·»åŠ æ—¶é—´ç»´åº¦ (å•å¸§)
  .to(...)              : [1, 3, 1, 480, 832]     # è½¬ç§»åˆ° GPU + bfloat16
```

```python
    # Encode the input image as the first latent
    initial_latent = pipeline.vae.encode_to_latent(image).to(device=device, dtype=torch.bfloat16)
    initial_latent = initial_latent.repeat(args.num_samples, 1, 1, 1, 1)
```

**VAE ç¼–ç **:
```
image                   : [1, 3, 1, 480, 832]     # åƒç´ ç©ºé—´
  â†’ VAE.encode          : [1, 1, 16, 60, 104]     # æ½œåœ¨ç©ºé—´ (å‹ç¼© 8x8, é€šé“ 3â†’16)
  .repeat(num_samples)  : [B, 1, 16, 60, 104]     # å¤åˆ¶ B ä»½
```

```python
    sampled_noise = torch.randn(
        [args.num_samples, args.num_output_frames - 1, 16, 60, 104], device=device, dtype=torch.bfloat16
    )
```

**å™ªå£°å½¢çŠ¶**: `[B, 20, 16, 60, 104]`
- I2V æ¨¡å¼ä¸‹ï¼Œç¬¬ä¸€å¸§æ˜¯è¾“å…¥å›¾åƒï¼Œåªéœ€ç”Ÿæˆåç»­ 20 å¸§
- 21 - 1 = 20 å¸§å™ªå£°

---

### L151-163: æ–‡ç”Ÿè§†é¢‘ (T2V) åˆ†æ”¯

```python
else:
    # For text-to-video, batch is just the text prompt
    prompt = batch['prompts'][0]
    extended_prompt = batch['extended_prompts'][0] if 'extended_prompts' in batch else None
    if extended_prompt is not None:
        prompts = [extended_prompt] * args.num_samples
    else:
        prompts = [prompt] * args.num_samples
    initial_latent = None
```

- ä¼˜å…ˆä½¿ç”¨æ‰©å±•æç¤ºï¼ˆextended_promptï¼Œé€šå¸¸ç”± GPT-4 ç­‰æ‰©å±•ç”Ÿæˆï¼Œæ›´è¯¦ç»†ï¼‰
- å¦‚æœæ²¡æœ‰æ‰©å±•æç¤ºï¼Œä½¿ç”¨åŸå§‹æç¤º
- T2V æ— åˆå§‹å¸§ï¼Œ`initial_latent = None`

```python
    sampled_noise = torch.randn(
        [args.num_samples, args.num_output_frames, 16, 60, 104], device=device, dtype=torch.bfloat16
    )
```

**å™ªå£°å½¢çŠ¶**: `[B, 21, 16, 60, 104]`
- T2V éœ€è¦ç”Ÿæˆå…¨éƒ¨ 21 å¸§
- å„ç»´åº¦å«ä¹‰ï¼š
  - `B`: batch sizeï¼ˆ`num_samples`ï¼‰
  - `21`: æ½œåœ¨å¸§æ•°
  - `16`: æ½œåœ¨é€šé“æ•°
  - `60`: æ½œåœ¨é«˜åº¦ï¼ˆ480 Ã· 8ï¼‰
  - `104`: æ½œåœ¨å®½åº¦ï¼ˆ832 Ã· 8ï¼‰

---

### L165-172: æ ¸å¿ƒæ¨ç†è°ƒç”¨

```python
# Generate 81 frames
video, latents = pipeline.inference(
    noise=sampled_noise,
    text_prompts=prompts,
    return_latents=True,
    initial_latent=initial_latent,
    low_memory=low_memory,
)
```

**å‚æ•°è¯´æ˜**ï¼š
| å‚æ•° | ç±»å‹ | å½¢çŠ¶/å€¼ | è¯´æ˜ |
|------|------|---------|------|
| `noise` | Tensor | `[B, 21, 16, 60, 104]` | åˆå§‹é«˜æ–¯å™ªå£° |
| `text_prompts` | List[str] | é•¿åº¦ B | æ–‡æœ¬æç¤ºåˆ—è¡¨ |
| `return_latents` | bool | True | æ˜¯å¦è¿”å›æ½œåœ¨ç©ºé—´ç»“æœ |
| `initial_latent` | Tensor/None | `[B, 1, 16, 60, 104]` or None | I2V çš„ç¬¬ä¸€å¸§ |
| `low_memory` | bool | True/False | æ˜¯å¦å¯ç”¨ä½æ˜¾å­˜æ¨¡å¼ |

**è¿”å›å€¼**ï¼š
| è¿”å›å€¼ | å½¢çŠ¶ | è¯´æ˜ |
|--------|------|------|
| `video` | `[B, 81, 3, 480, 832]` | åƒç´ ç©ºé—´è§†é¢‘ï¼ˆå½’ä¸€åŒ–åˆ° [0,1]ï¼‰ |
| `latents` | `[B, 21, 16, 60, 104]` | æ½œåœ¨ç©ºé—´è¡¨ç¤º |

> **å¸§æ•°å…³ç³»**: 21 å¸§æ½œåœ¨ç©ºé—´ Ã— 4ï¼ˆVAE æ—¶é—´ä¸Šé‡‡æ ·ï¼‰= 84 å¸§ï¼Œä½†è¾¹ç•Œå¤„ç†åè¾“å‡º 81 å¸§

---

### L173-175: åå¤„ç†

```python
current_video = rearrange(video, 'b t c h w -> b t h w c').cpu()
all_video.append(current_video)
num_generated_frames += latents.shape[1]
```

**ç»´åº¦å˜æ¢**ï¼š
```
video                   : [B, 81, 3, 480, 832]    # PyTorch æ ¼å¼: C åœ¨å‰
  â†’ rearrange           : [B, 81, 480, 832, 3]    # OpenCV/è§†é¢‘æ ¼å¼: C åœ¨å
  .cpu()                : è½¬ç§»åˆ° CPU å†…å­˜
```

---

### L177-178: æœ€ç»ˆè§†é¢‘å‡†å¤‡

```python
# Final output video
video = 255.0 * torch.cat(all_video, dim=1)
```

- `torch.cat(all_video, dim=1)`: æ²¿æ—¶é—´ç»´åº¦æ‹¼æ¥ï¼ˆå½“å‰åªæœ‰ä¸€æ®µï¼‰
- `* 255.0`: ä» [0, 1] ç¼©æ”¾åˆ° [0, 255]ï¼ˆè§†é¢‘å­˜å‚¨éœ€è¦ uint8ï¼‰

---

### L180-181: æ¸…ç†ç¼“å­˜

```python
# Clear VAE cache
pipeline.vae.model.clear_cache()
```

VAE è§£ç å™¨ä½¿ç”¨å› æœå·ç§¯ï¼Œä¼šç¼“å­˜ä¹‹å‰å¸§çš„ç‰¹å¾ã€‚æ¯ä¸ªæ ·æœ¬å¤„ç†å®Œåæ¸…ç†ï¼Œé¿å…å†…å­˜æ³„æ¼ã€‚

---

### L183-192: ä¿å­˜è§†é¢‘

```python
# Save the video if the current prompt is not a dummy prompt
if idx < num_prompts:
```

åˆ†å¸ƒå¼è®­ç»ƒæ—¶å¯èƒ½æœ‰å¡«å……çš„ dummy æ•°æ®ï¼Œåªä¿å­˜æœ‰æ•ˆæ ·æœ¬ã€‚

```python
    model = "regular" if not args.use_ema else "ema"
    for seed_idx in range(args.num_samples):
        # All processes save their videos
        if args.save_with_index:
            output_path = os.path.join(args.output_folder, f'{idx}-{seed_idx}_{model}.mp4')
        else:
            output_path = os.path.join(args.output_folder, f'{prompt[:100]}-{seed_idx}.mp4')
        write_video(output_path, video[seed_idx], fps=16)
```

**æ–‡ä»¶å‘½å**ï¼š
| æ¨¡å¼ | ç¤ºä¾‹æ–‡ä»¶å |
|------|-----------|
| `--save_with_index` | `0-0_ema.mp4`, `0-1_ema.mp4` |
| é»˜è®¤ï¼ˆæç¤ºæ–‡æœ¬ï¼‰ | `A cat playing with...-0.mp4` |

**`write_video` å‚æ•°**ï¼š
- `output_path`: è¾“å‡ºè·¯å¾„
- `video[seed_idx]`: å½¢çŠ¶ `[81, 480, 832, 3]`ï¼Œå€¼èŒƒå›´ [0, 255]
- `fps=16`: å¸§ç‡ 16 FPSï¼Œ81 å¸§ â‰ˆ **5 ç§’è§†é¢‘**

---

## å¼ é‡å½¢çŠ¶å®Œæ•´è¿½è¸ª

```
è¾“å…¥æç¤º: "A cat playing with a ball"
     â†“
æ–‡æœ¬ç¼–ç å™¨ (UMT5-XXL)
     â†“
prompt_embeds: [1, 512, 4096]
     â†“
éšæœºå™ªå£°: [1, 21, 16, 60, 104]
     â†“
æ‰©æ•£ç”Ÿæˆå™¨ (CausalWanModel) Ã— 4 æ­¥å»å™ª
     â†“
æ½œåœ¨ç©ºé—´: [1, 21, 16, 60, 104]
     â†“
VAE è§£ç å™¨
     â†“
åƒç´ ç©ºé—´: [1, 81, 3, 480, 832]
     â†“
rearrange + Ã— 255
     â†“
è§†é¢‘: [81, 480, 832, 3] (uint8)
     â†“
write_video
     â†“
output.mp4 (480Ã—832, 81å¸§, 16fps, ~5ç§’)
```

---

## ç¬¬åäºŒéƒ¨åˆ†ï¼šè§†é¢‘ä¿å­˜ (L177-192)

```python
# Final output video
video = 255.0 * torch.cat(all_video, dim=1)

# Clear VAE cache
pipeline.vae.model.clear_cache()

# Save the video if the current prompt is not a dummy prompt
if idx < num_prompts:
    model = "regular" if not args.use_ema else "ema"
    for seed_idx in range(args.num_samples):
        if args.save_with_index:
            output_path = os.path.join(args.output_folder, f'{idx}-{seed_idx}_{model}.mp4')
        else:
            output_path = os.path.join(args.output_folder, f'{prompt[:100]}-{seed_idx}.mp4')
        write_video(output_path, video[seed_idx], fps=16)
```

### è§£é‡Š
1. **åƒç´ å€¼ç¼©æ”¾**ï¼šä» `[0, 1]` ç¼©æ”¾åˆ° `[0, 255]`
2. **æ¸…ç†ç¼“å­˜**ï¼šé‡Šæ”¾ VAE è§£ç å™¨çš„ç¼“å­˜
3. **ä¿å­˜è§†é¢‘**ï¼š
   - å¸§ç‡ï¼š16 FPS
   - æ–‡ä»¶åï¼šç´¢å¼•æ¨¡å¼ `0-0_ema.mp4` æˆ–æç¤ºæ¨¡å¼ `prompt[:100]-0.mp4`

---

## ğŸ“Š æ•°æ®æµæ€»ç»“

```
è¾“å…¥æ–‡æœ¬æç¤º
    â†“
TextDataset åŠ è½½
    â†“
ç”Ÿæˆéšæœºå™ªå£° [B, 21, 16, 60, 104]
    â†“
pipeline.inference()
    â”œâ”€â”€ text_encoder: æ–‡æœ¬ â†’ åµŒå…¥ [B, 512, 4096]
    â”œâ”€â”€ generator: å™ªå£° â†’ æ½œåœ¨ç©ºé—´ [B, 21, 16, 60, 104]
    â””â”€â”€ vae.decode: æ½œåœ¨ç©ºé—´ â†’ åƒç´  [B, 81, 3, 480, 832]
    â†“
rearrange: [B, T, C, H, W] â†’ [B, T, H, W, C]
    â†“
Ã— 255.0
    â†“
write_video: ä¿å­˜ä¸º MP4
```

---

## ğŸš€ è¿è¡Œç¤ºä¾‹

```bash
python inference.py \
    --config_path configs/self_forcing_dmd.yaml \
    --checkpoint_path checkpoints/self_forcing_dmd.pt \
    --data_path prompts/MovieGenVideoBench_extended.txt \
    --output_folder videos/output \
    --use_ema \
    --num_samples 2
```
